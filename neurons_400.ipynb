{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neurons 400.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "a1fed70d-8e6b-45e6-fd70-fc70454a8cdc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "558b247e-7d27-4a0a-e689-fcd14cd0e501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "cc31e113-8e92-483a-db74-a0f6a8ccfc79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "36b76ee9-ee81-4d64-a000-5746572fbe72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "59e3d80f-f4d9-4803-9876-408b20e5a011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -13.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "d6d9741d-649b-4cc3-fe15-83f244cb2740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.970199\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.970497\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.970792\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.971084\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.971373\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.961660\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.962043\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.962423\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.962798\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.963170\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.963539\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.963903\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.964264\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.964622\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.964975\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.965326\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.955672\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.946116\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.946654\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.927188\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.927916\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.928637\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.929350\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.930057\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.920756\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.921549\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.922333\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.923110\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.923879\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.914640\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.905494\n",
            "resetting env. episode 36.000000, reward total was -18.000000. running mean: -20.876439\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.877674\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.868898\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.860209\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.861607\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.862991\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.864361\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.865717\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -20.837060\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.828689\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.830402\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.832098\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.823777\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.805540\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.787484\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.789609\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.791713\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.793796\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.775858\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.778100\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.780319\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.782515\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.784690\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.786843\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.778975\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.781185\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.783373\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.785540\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.787684\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.789807\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.781909\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.784090\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.766249\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.758587\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.761001\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.763391\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.765757\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.768099\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.770418\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.772714\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.764987\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.757337\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.759764\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.752166\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.754645\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.757098\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.759527\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.761932\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.754313\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.756769\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.749202\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.751710\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.754193\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.746651\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.749184\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.751692\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.754175\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.746634\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.739167\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.741776\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.744358\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.736914\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.739545\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.722150\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.724928\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.727679\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.730402\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.723098\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.725867\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.718608\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.711422\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.714308\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.697165\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.700193\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.693191\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.696260\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.689297\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.682404\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.665580\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.668924\n",
            "resetting env. episode 116.000000, reward total was -18.000000. running mean: -20.642235\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.645813\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.649354\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.652861\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.646332\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.639869\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.643470\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.647036\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.650565\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.654060\n",
            "resetting env. episode 126.000000, reward total was -18.000000. running mean: -20.627519\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.631244\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.634931\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.638582\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.642196\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.645774\n",
            "resetting env. episode 132.000000, reward total was -17.000000. running mean: -20.609317\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.603223\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.597191\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.601219\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.605207\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.599155\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.593163\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.597232\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.591259\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.585347\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.579493\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.583698\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.587861\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.581983\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.586163\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.590301\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.594398\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.598454\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.602470\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.596445\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.600481\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.604476\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.598431\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.602447\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.606422\n",
            "resetting env. episode 157.000000, reward total was -18.000000. running mean: -20.580358\n",
            "resetting env. episode 158.000000, reward total was -18.000000. running mean: -20.554555\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.549009\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.543519\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.538084\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.542703\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.547276\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.551803\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.546285\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.550822\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.545314\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.549861\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.534362\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.539019\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.533628\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.538292\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.532909\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -20.517580\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.522404\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.527180\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.521908\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.526689\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.521423\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.526208\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.520946\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.525737\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.520479\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.525275\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.520022\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.514822\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.519673\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.514477\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.519332\n",
            "resetting env. episode 190.000000, reward total was -18.000000. running mean: -20.494139\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.489197\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.494305\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.499362\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.504369\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.509325\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.514232\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.519089\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.523898\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.528659\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.533373\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.538039\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.532659\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.527332\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.522059\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.526838\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.531570\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.516254\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.521092\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.515881\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.500722\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.495715\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.500758\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.505750\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.510692\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.515586\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.520430\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.515225\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.520073\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.524872\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.519624\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.524427\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.519183\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.523991\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.528751\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.533464\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.538129\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.542748\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.537320\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.531947\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.536628\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.541262\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.535849\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.540490\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.545086\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.549635\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.544138\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.538697\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.543310\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.547877\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.542398\n",
            "resetting env. episode 241.000000, reward total was -18.000000. running mean: -20.516974\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.521804\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.526586\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.511320\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.506207\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.501145\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.496134\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.491172\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.496261\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.491298\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.496385\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.501421\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.506407\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.511343\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.516230\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.521067\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.525857\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.520598\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.525392\n",
            "resetting env. episode 260.000000, reward total was -18.000000. running mean: -20.500138\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.505137\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.500085\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.495084\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.500134\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.495132\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.480181\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.485379\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.470525\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.475820\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.481062\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.486251\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.491389\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.486475\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.481610\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.476794\n",
            "resetting env. episode 276.000000, reward total was -17.000000. running mean: -20.442026\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.447606\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.453130\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.448598\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.454113\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.439571\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.445176\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.440724\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.446317\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.451854\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.437335\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.442962\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.448532\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.454047\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.459506\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.454911\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.450362\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.455858\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.441300\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.446887\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.442418\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.447994\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.453514\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.458979\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.464389\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.469745\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.475048\n",
            "resetting env. episode 303.000000, reward total was -18.000000. running mean: -20.450297\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.445794\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.441336\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.446923\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.442454\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.448029\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.453549\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.459013\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.464423\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.469779\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.455081\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.440530\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.446125\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.441664\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.447247\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.452775\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.458247\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.433664\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.429328\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.435035\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.440684\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.446277\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.451815\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.447296\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.432823\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.438495\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.444110\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.439669\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.445272\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.450820\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.456312\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.461748\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.467131\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.462460\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.457835\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.463257\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.468624\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.453938\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.459399\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.454805\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.450256\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.455754\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.451196\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.446684\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.442218\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.437795\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.443417\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.438983\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.444593\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.450147\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.455646\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.461090\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.456479\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.461914\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.467295\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.462622\n",
            "resetting env. episode 359.000000, reward total was -18.000000. running mean: -20.437996\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.443616\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.449179\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.444688\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.450241\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.435738\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.441381\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.436967\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.442598\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.438172\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.423790\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.429552\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.415256\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.421104\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.416893\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.412724\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.418597\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.424411\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.430167\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.435865\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.441506\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.447091\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.452620\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.448094\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.443613\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.439177\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.444785\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.450337\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.455834\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.441276\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.436863\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.442494\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.438069\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.423689\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.429452\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.435157\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.430806\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.436498\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.432133\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.437811\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.443433\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.448999\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.444509\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.450064\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.455563\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.451008\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.456497\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.461932\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.467313\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.472640\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.477914\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.483134\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.478303\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.463520\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.468885\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.474196\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.479454\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.474660\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.469913\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.465214\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.470562\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.465856\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.471198\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.466486\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.471821\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.457102\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.462531\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.467906\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.473227\n",
            "resetting env. episode 428.000000, reward total was -16.000000. running mean: -20.428495\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.434210\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.439868\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.445469\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.451014\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.446504\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.452039\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.457519\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.462944\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.458314\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.463731\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.459094\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.464503\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.469858\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.475159\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.470408\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.475704\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.470946\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.476237\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.471475\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.466760\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.472092\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.477371\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.472598\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.467872\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.453193\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.458661\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.464074\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.469434\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.474739\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.479992\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.485192\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.490340\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.495437\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.500482\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.485478\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.480623\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.475817\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.481058\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.476248\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.481485\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.476670\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.471904\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.477185\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.482413\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.477589\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.482813\n",
            "resetting env. episode 475.000000, reward total was -17.000000. running mean: -20.447985\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.433505\n",
            "resetting env. episode 477.000000, reward total was -18.000000. running mean: -20.409170\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.405078\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.401027\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.397017\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.393047\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.389116\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.385225\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.391373\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.387459\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.393585\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.389649\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.395752\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.401795\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.407777\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.403699\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.409662\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.405565\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.391510\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.397595\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.383619\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.379783\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.385985\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.392125\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.388204\n",
            "CPU times: user 33min 57s, sys: 10min 58s, total: 44min 56s\n",
            "Wall time: 23min 15s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "5cf131d9-a77a-4f60-fab8-17ae09e973c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980496\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980691\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.970884\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.961175\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.961564\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.941948\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.942528\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.933103\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.933772\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.934434\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.925090\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.925839\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.906581\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.907515\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.908440\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.889355\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.890462\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.881557\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.872742\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.854014\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.855474\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.836919\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.818550\n",
            "resetting env. episode 29.000000, reward total was -18.000000. running mean: -20.790365\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.792461\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.784536\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.786691\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.788824\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.790936\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.793027\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.785096\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.787245\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.789373\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.791479\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.793564\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.795629\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.797672\n",
            "resetting env. episode 43.000000, reward total was -19.000000. running mean: -20.779696\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.781899\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.784080\n",
            "resetting env. episode 46.000000, reward total was -18.000000. running mean: -20.756239\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.758677\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.741090\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.733679\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.726342\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.729079\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.721788\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.724570\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.717324\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.720151\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.722950\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.725720\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.708463\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.711378\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.714264\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.707122\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.700051\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.703050\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.696020\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.699059\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.692069\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.695148\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.698197\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.701215\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.704203\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.707160\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.700089\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.703088\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.696057\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.689097\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.692206\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.695284\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.698331\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.701347\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.704334\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.697291\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.690318\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.683414\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.686580\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.689715\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.682817\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.675989\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.679229\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.682437\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.665613\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.668957\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.672267\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.665544\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.668889\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.672200\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.675478\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.678723\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.681936\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.675117\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.678365\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.681582\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.684766\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.667918\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.661239\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.654627\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.658080\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.641500\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.645085\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.638634\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.642247\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.645825\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.639367\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.642973\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.646543\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.630078\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.623777\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.617539\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.611364\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.605250\n",
            "resetting env. episode 120.000000, reward total was -18.000000. running mean: -20.579198\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.583406\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.587572\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.591696\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.585779\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.589921\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.594022\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.588082\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.582201\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.576379\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.560615\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -20.545009\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.549559\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.554063\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -20.528523\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.533238\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.537905\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.542526\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.547101\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.541630\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.526214\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.520951\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.525742\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.530485\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -20.505180\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.510128\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.515027\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.519876\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.524678\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.519431\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.504236\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.509194\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.514102\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.518961\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.513772\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.508634\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.513547\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.518412\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.523228\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.527996\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.532716\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.537388\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.522015\n",
            "resetting env. episode 163.000000, reward total was -18.000000. running mean: -20.496794\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.491827\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.486908\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.492039\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.487119\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.472248\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.477525\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.472750\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.468022\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.473342\n",
            "resetting env. episode 173.000000, reward total was -18.000000. running mean: -20.448609\n",
            "resetting env. episode 174.000000, reward total was -18.000000. running mean: -20.424123\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.429881\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.425583\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.421327\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.427113\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.432842\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.428514\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.434229\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.419887\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.425688\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.431431\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.437116\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.442745\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.448318\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.453835\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.459296\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.454703\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.450156\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.455655\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.451098\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.446587\n",
            "resetting env. episode 195.000000, reward total was -18.000000. running mean: -20.422121\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.407900\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.413821\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.399683\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.405686\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.401629\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.407613\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.403537\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.409501\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.415406\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.401252\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.407240\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.413167\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.419036\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.424845\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.430597\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.436291\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.441928\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.437509\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.443134\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.438702\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.444315\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.449872\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.455373\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.460820\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.446212\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.451749\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.447232\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.432760\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.438432\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.434048\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.439707\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.425310\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.431057\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.416746\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.412579\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.408453\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.414369\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.410225\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.416123\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.421962\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.407742\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.413664\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.419528\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.415333\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.421179\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.426967\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.432698\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.428371\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.434087\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.439746\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.425349\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.431095\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.436784\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.442416\n",
            "resetting env. episode 250.000000, reward total was -17.000000. running mean: -20.407992\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.403912\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.409873\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.415775\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.411617\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.417501\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.423326\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.429092\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.424801\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.430553\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.436248\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.421885\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.427667\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.433390\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.439056\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.444665\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.450219\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.445717\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.451259\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.456747\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.462179\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.467558\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.472882\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.468153\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.473472\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.458737\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.454150\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.459608\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.455012\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.460462\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.455857\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.461299\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.466686\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.472019\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.467299\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.452626\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.448099\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.453618\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.449082\n",
            "resetting env. episode 289.000000, reward total was -18.000000. running mean: -20.424591\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.420345\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.426142\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.431881\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.427562\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.423286\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.419053\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.414863\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.420714\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.426507\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.422242\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.428020\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.433739\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.439402\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.445008\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.440558\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.446152\n",
            "resetting env. episode 306.000000, reward total was -18.000000. running mean: -20.421691\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.427474\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.433199\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.438867\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.434478\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.440134\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.445732\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.441275\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.436862\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.442494\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.448069\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.443588\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.439152\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.444761\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.450313\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.455810\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.461252\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.456639\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.462073\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.447452\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.442978\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.438548\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.444162\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.449721\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.455224\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.460671\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.456065\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.441504\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.447089\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.452618\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.458092\n",
            "resetting env. episode 337.000000, reward total was -18.000000. running mean: -20.433511\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.439176\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.444784\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.440336\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.435933\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.431573\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.417258\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.403085\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.409054\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.414964\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.420814\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.416606\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.422440\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.428216\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.433933\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.439594\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.435198\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.440846\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.426438\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.412173\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.408052\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.403971\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.409931\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.415832\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.411674\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.417557\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.423381\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.409148\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.415056\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.420906\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.426696\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.432430\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.438105\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.433724\n",
            "resetting env. episode 371.000000, reward total was -18.000000. running mean: -20.409387\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.405293\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.401240\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.387228\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.373355\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.369622\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.375926\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.372166\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.368445\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.364760\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.371113\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.377402\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.383628\n",
            "resetting env. episode 384.000000, reward total was -17.000000. running mean: -20.349791\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.356293\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.342730\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.339303\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.335910\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.342551\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.339125\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.345734\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.342277\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.338854\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.335466\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.342111\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.328690\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.335403\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.332049\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.338728\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.335341\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.341988\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.328568\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.335282\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.341929\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.348510\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.345025\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.351575\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.338059\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.344678\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.351232\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.337719\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.334342\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.320999\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.327789\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.334511\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.341166\n",
            "resetting env. episode 417.000000, reward total was -18.000000. running mean: -20.317754\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.324576\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.331331\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.328017\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.324737\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.321490\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.328275\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.334992\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.341642\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.348226\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.344744\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.351296\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.337783\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.344405\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.350961\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.337452\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.344077\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.350636\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.347130\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.343659\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.350222\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.346720\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.333253\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.339920\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.346521\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.343056\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.339625\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.346229\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.342767\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.329339\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.336046\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.342685\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.339258\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.345866\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.352407\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.358883\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.365294\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.361641\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.348025\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.354545\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.350999\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.357489\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.363914\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.370275\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.376572\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.382807\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.368979\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.375289\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.371536\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.367821\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.374142\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.360401\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.346797\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.343329\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.349896\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.356397\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.352833\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.339304\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.335911\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.332552\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.329227\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.335934\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.332575\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.339249\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.335857\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.322498\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.319273\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.326081\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.332820\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.339492\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.336097\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.332736\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.339408\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.346014\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.332554\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.329229\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.335936\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.312577\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.319451\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.326257\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.322994\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.329764\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.336466\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.343102\n",
            "CPU times: user 34min 21s, sys: 11min 3s, total: 45min 24s\n",
            "Wall time: 23min 29s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "5f7c7b86-1e82-495f-812a-2c2a98ad0782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "fcf1bc60-41c3-409e-fd2e-a5865b7df94b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.970299\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.960596\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.950990\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.951480\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951965\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.952446\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.942921\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.943492\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.934057\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.934717\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.925369\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -20.896116\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.897155\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.888183\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.879301\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.870508\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.871803\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.863085\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -20.844454\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.836010\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.837650\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.839273\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.840880\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.842472\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.844047\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.835606\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.827250\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.818978\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.820788\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.822580\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.824354\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.816111\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.817950\n",
            "resetting env. episode 37.000000, reward total was -19.000000. running mean: -20.799770\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.801772\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.803755\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.785717\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.777860\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.770081\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.772381\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.774657\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.756910\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.739341\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.741948\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.744528\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.747083\n",
            "resetting env. episode 50.000000, reward total was -18.000000. running mean: -20.719612\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.722416\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.705192\n",
            "resetting env. episode 53.000000, reward total was -18.000000. running mean: -20.678140\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.671359\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.674645\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.677898\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.681119\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.674308\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.667565\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.670890\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.664181\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.667539\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.670863\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.674155\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.657413\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.650839\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.654331\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.647787\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.641310\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.644896\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.638448\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.632063\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.635742\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.629385\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.623091\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.626860\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.630592\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.634286\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.627943\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.631663\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.625347\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -20.609093\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.613002\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.606872\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.610804\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.604696\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.608649\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.602562\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.596537\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.590571\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.574665\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.578919\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.563130\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.557498\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.561923\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.566304\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.570641\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.564935\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.569285\n",
            "resetting env. episode 100.000000, reward total was -18.000000. running mean: -20.543592\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.548157\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.552675\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -20.527148\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.521877\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.526658\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.511391\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.516277\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.521115\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.525904\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.530645\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.525338\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.520085\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.514884\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.519735\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.524538\n",
            "resetting env. episode 116.000000, reward total was -19.000000. running mean: -20.509292\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.514199\n",
            "resetting env. episode 118.000000, reward total was -18.000000. running mean: -20.489057\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.484167\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.489325\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.474432\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.469688\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.454991\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.450441\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.455936\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.461377\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.466763\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.472096\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.457375\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.452801\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.458273\n",
            "resetting env. episode 132.000000, reward total was -15.000000. running mean: -20.403690\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.389653\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.395757\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.401799\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.397781\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.393803\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.389865\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.395967\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.392007\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.378087\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.374306\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.380563\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.366757\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.373090\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.379359\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.385565\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -20.371710\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.377993\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.374213\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.360471\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.356866\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.353297\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.349764\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.356267\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.342704\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.339277\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.335884\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.342525\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.349100\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.345609\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.342153\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.328731\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.325444\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.322190\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.328968\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.325678\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.332421\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.339097\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.345706\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.342249\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.338827\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.345438\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.341984\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.348564\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.355078\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.361528\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.367912\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.374233\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.380491\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.386686\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.382819\n",
            "resetting env. episode 183.000000, reward total was -18.000000. running mean: -20.358991\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.365401\n",
            "resetting env. episode 185.000000, reward total was -18.000000. running mean: -20.341747\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.338330\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.344946\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.351497\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.347982\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.334502\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.331157\n",
            "resetting env. episode 192.000000, reward total was -17.000000. running mean: -20.297845\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.294867\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.281918\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.289099\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.276208\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.283446\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.290612\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.277705\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.274928\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.262179\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.269557\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.266862\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.274193\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.281451\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.278637\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.285850\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.282992\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.290162\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.287260\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.294388\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.291444\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.278529\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.275744\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.282987\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.270157\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.277455\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.284681\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.291834\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.298915\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.295926\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.292967\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.290037\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.297137\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.304166\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.301124\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -20.278113\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.275332\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.282578\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.269753\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.277055\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.274284\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.281542\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.278726\n",
            "resetting env. episode 235.000000, reward total was -17.000000. running mean: -20.245939\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.243480\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.231045\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.228734\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.226447\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.234182\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.231841\n",
            "resetting env. episode 242.000000, reward total was -18.000000. running mean: -20.209522\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.217427\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.205253\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.203200\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.211168\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.219057\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.226866\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.234597\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.242251\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.249829\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.257331\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.244757\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.242310\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.249887\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.257388\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.264814\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.272166\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.259444\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.256850\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.254281\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.251738\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.249221\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.256729\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.264161\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.271520\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.268805\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.276117\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.283355\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.270522\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.277817\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.275038\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.262288\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.269665\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.276969\n",
            "resetting env. episode 276.000000, reward total was -18.000000. running mean: -20.254199\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.251657\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.259140\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.266549\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.273883\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.281145\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.278333\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.285550\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.292694\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.299767\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.306770\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.313702\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.320565\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.327359\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.334086\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.330745\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.317437\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.324263\n",
            "resetting env. episode 294.000000, reward total was -18.000000. running mean: -20.301020\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.298010\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.295030\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.302080\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.309059\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.315968\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.322809\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.319581\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.326385\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.333121\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.339790\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.346392\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.352928\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.349399\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.335905\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.342546\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.349120\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.345629\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.352173\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.338651\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.335264\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.331912\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.338593\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.345207\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.341755\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.348337\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.354854\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.361305\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.357692\n",
            "resetting env. episode 323.000000, reward total was -18.000000. running mean: -20.334115\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.340774\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.337366\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.323993\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.330753\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.327445\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.334171\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.340829\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.347421\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.353947\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.360407\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.356803\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.353235\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.339703\n",
            "resetting env. episode 337.000000, reward total was -17.000000. running mean: -20.306306\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.303243\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.310210\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.317108\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.323937\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.330698\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.327391\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.334117\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.320776\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.327568\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.334292\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.340949\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.347540\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.354064\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.360524\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.346918\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.333449\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.330115\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.326814\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.323545\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -20.300310\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.307307\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.314234\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.321092\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.327881\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.324602\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.331356\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.328042\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.334762\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.331414\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.328100\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.334819\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.331471\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.338156\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.344775\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.351327\n",
            "resetting env. episode 373.000000, reward total was -16.000000. running mean: -20.307814\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.314735\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.311588\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.308472\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.315387\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.322234\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.329011\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.325721\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.312464\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.299339\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.286346\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.283482\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.270648\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.267941\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.275262\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.282509\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.279684\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.286887\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.294018\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.291078\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.278167\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.275386\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.282632\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.289806\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.286907\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.284038\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.291198\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.298286\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.305303\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.292250\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.279328\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.286534\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.273669\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.270932\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.278223\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.275441\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.272686\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.259959\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.257360\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.254786\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.252238\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.259716\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.257119\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.244548\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.232102\n",
            "resetting env. episode 418.000000, reward total was -17.000000. running mean: -20.199781\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.197783\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.205806\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.213747\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.211610\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.209494\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.207399\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.195325\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.193372\n",
            "resetting env. episode 427.000000, reward total was -18.000000. running mean: -20.171438\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.179724\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.187926\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.196047\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.194087\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.202146\n",
            "resetting env. episode 433.000000, reward total was -18.000000. running mean: -20.180124\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.168323\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.156640\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.165073\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.163423\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.171789\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.170071\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.158370\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.166786\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.165118\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.173467\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.171733\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.180015\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.178215\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.186433\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.194569\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.192623\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.190697\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.198790\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.196802\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.204834\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.192785\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.180858\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.189049\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.187158\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.195287\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.183334\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.191501\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.199586\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.207590\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.195514\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.183559\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.181723\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.169906\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.178207\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.176425\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.174661\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.182914\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.191085\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.199174\n",
            "resetting env. episode 473.000000, reward total was -17.000000. running mean: -20.167182\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.175510\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.183755\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.171918\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.180199\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.188397\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.196513\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.194548\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.202602\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.210576\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.218470\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.226286\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.234023\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.241682\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.239266\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.246873\n",
            "resetting env. episode 489.000000, reward total was -16.000000. running mean: -20.204404\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.202360\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.210337\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.218233\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.216051\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.223890\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.231652\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.229335\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.217042\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.224871\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.232623\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.230296\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.227993\n",
            "resetting env. episode 502.000000, reward total was -17.000000. running mean: -20.195713\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.203756\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.201719\n",
            "resetting env. episode 505.000000, reward total was -18.000000. running mean: -20.179702\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.177905\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -20.166125\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.164464\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.162820\n",
            "resetting env. episode 510.000000, reward total was -19.000000. running mean: -20.151191\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.149679\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.148183\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.156701\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.155134\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.143582\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.142147\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.140725\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.129318\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.138025\n",
            "resetting env. episode 520.000000, reward total was -19.000000. running mean: -20.126645\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.135378\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.144024\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.142584\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.151158\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.149647\n",
            "resetting env. episode 526.000000, reward total was -19.000000. running mean: -20.138150\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.146769\n",
            "resetting env. episode 528.000000, reward total was -19.000000. running mean: -20.135301\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.133948\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.142608\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.151182\n",
            "resetting env. episode 532.000000, reward total was -17.000000. running mean: -20.119671\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.128474\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.137189\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.135817\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.134459\n",
            "resetting env. episode 537.000000, reward total was -19.000000. running mean: -20.123114\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.121883\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.120664\n",
            "resetting env. episode 540.000000, reward total was -19.000000. running mean: -20.109458\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.118363\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.117180\n",
            "resetting env. episode 543.000000, reward total was -19.000000. running mean: -20.106008\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.104948\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.113898\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.112759\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -20.101632\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -20.090615\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.099709\n",
            "resetting env. episode 550.000000, reward total was -19.000000. running mean: -20.088712\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.097825\n",
            "resetting env. episode 552.000000, reward total was -18.000000. running mean: -20.076847\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.066078\n",
            "resetting env. episode 554.000000, reward total was -18.000000. running mean: -20.045418\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.054963\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.054414\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.053870\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.063331\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.072698\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.071971\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.071251\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.080538\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.089733\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.088836\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.087947\n",
            "resetting env. episode 566.000000, reward total was -19.000000. running mean: -20.077068\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.076297\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.085534\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.084679\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.093832\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.102894\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.101865\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.110846\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.119738\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.128540\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.137255\n",
            "resetting env. episode 577.000000, reward total was -19.000000. running mean: -20.125882\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -20.124624\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.123377\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -20.112144\n",
            "resetting env. episode 581.000000, reward total was -18.000000. running mean: -20.091022\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.100112\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.109111\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.108020\n",
            "resetting env. episode 585.000000, reward total was -19.000000. running mean: -20.096939\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.095970\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.095010\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.104060\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.103020\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.111989\n",
            "resetting env. episode 591.000000, reward total was -19.000000. running mean: -20.100870\n",
            "resetting env. episode 592.000000, reward total was -17.000000. running mean: -20.069861\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.079162\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.088371\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.087487\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.086612\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.085746\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.084888\n",
            "resetting env. episode 599.000000, reward total was -19.000000. running mean: -20.074040\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.083299\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.092466\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.101542\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.110526\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.109421\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.118327\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.117143\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.125972\n",
            "resetting env. episode 608.000000, reward total was -19.000000. running mean: -20.114712\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.113565\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.122429\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -20.111205\n",
            "resetting env. episode 612.000000, reward total was -18.000000. running mean: -20.090093\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.089192\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.098300\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.107317\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.096244\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.095282\n",
            "resetting env. episode 618.000000, reward total was -20.000000. running mean: -20.094329\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.103386\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.102352\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.111328\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.110215\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.119113\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -20.117922\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.126742\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.135475\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.144120\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.152679\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.141152\n",
            "resetting env. episode 630.000000, reward total was -18.000000. running mean: -20.119741\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.118543\n",
            "resetting env. episode 632.000000, reward total was -19.000000. running mean: -20.107358\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.116284\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.115121\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.113970\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.122831\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.121602\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.120386\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.129182\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.137891\n",
            "resetting env. episode 641.000000, reward total was -18.000000. running mean: -20.116512\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.115346\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.114193\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.123051\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.131821\n",
            "resetting env. episode 646.000000, reward total was -19.000000. running mean: -20.120502\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -20.119297\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.128104\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.136823\n",
            "resetting env. episode 650.000000, reward total was -19.000000. running mean: -20.125455\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.124201\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.122959\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.121729\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.120512\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.129307\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -20.118013\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.126833\n",
            "resetting env. episode 658.000000, reward total was -17.000000. running mean: -20.095565\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.104609\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.113563\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.122428\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.131203\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.139891\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.148492\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.157008\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.155437\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.143883\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.152444\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.160920\n",
            "resetting env. episode 670.000000, reward total was -19.000000. running mean: -20.149311\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.157817\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.156239\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.154677\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.153130\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.161599\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.149983\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.148483\n",
            "resetting env. episode 678.000000, reward total was -20.000000. running mean: -20.146998\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.145528\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.154073\n",
            "resetting env. episode 681.000000, reward total was -20.000000. running mean: -20.152532\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.151007\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.149497\n",
            "resetting env. episode 684.000000, reward total was -18.000000. running mean: -20.128002\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.116722\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.115555\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.114399\n",
            "resetting env. episode 688.000000, reward total was -18.000000. running mean: -20.093255\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.102323\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.111299\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.120186\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.118984\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.127795\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.136517\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -20.125151\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.123900\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.122661\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.121434\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -20.120220\n",
            "resetting env. episode 700.000000, reward total was -17.000000. running mean: -20.089018\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.098128\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.107146\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.106075\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.105014\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.103964\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.112924\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.111795\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.120677\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.129470\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.138176\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.146794\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.145326\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -20.133873\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.142534\n",
            "resetting env. episode 715.000000, reward total was -19.000000. running mean: -20.131109\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.139798\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.138400\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.147016\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.155545\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.163990\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.172350\n",
            "resetting env. episode 722.000000, reward total was -19.000000. running mean: -20.160627\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.169020\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.167330\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.165657\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.174000\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -20.172260\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.180538\n",
            "resetting env. episode 729.000000, reward total was -19.000000. running mean: -20.168732\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.167045\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.155375\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.163821\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.172183\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -20.160461\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.168856\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.177168\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.185396\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.193542\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.191607\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.189690\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.187794\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.195916\n",
            "resetting env. episode 743.000000, reward total was -19.000000. running mean: -20.183956\n",
            "resetting env. episode 744.000000, reward total was -18.000000. running mean: -20.162117\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.160496\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.158891\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.167302\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.175629\n",
            "resetting env. episode 749.000000, reward total was -18.000000. running mean: -20.153873\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.162334\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.160710\n",
            "resetting env. episode 752.000000, reward total was -18.000000. running mean: -20.139103\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.147712\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.146235\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.154773\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -20.143225\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.151793\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.160275\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.168672\n",
            "resetting env. episode 760.000000, reward total was -19.000000. running mean: -20.156985\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.165416\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.163761\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -20.162124\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.160503\n",
            "resetting env. episode 765.000000, reward total was -18.000000. running mean: -20.138898\n",
            "resetting env. episode 766.000000, reward total was -16.000000. running mean: -20.097509\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.086534\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.095668\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -20.084712\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.093864\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.102926\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.111896\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.120778\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.129570\n",
            "resetting env. episode 775.000000, reward total was -18.000000. running mean: -20.108274\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.117191\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.116019\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -20.104859\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -20.093811\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.092873\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.101944\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.100924\n",
            "resetting env. episode 783.000000, reward total was -20.000000. running mean: -20.099915\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.108916\n",
            "resetting env. episode 785.000000, reward total was -18.000000. running mean: -20.087827\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.086949\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.086079\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.085218\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.094366\n",
            "resetting env. episode 790.000000, reward total was -19.000000. running mean: -20.083422\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.082588\n",
            "resetting env. episode 792.000000, reward total was -19.000000. running mean: -20.071762\n",
            "resetting env. episode 793.000000, reward total was -18.000000. running mean: -20.051045\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.060534\n",
            "resetting env. episode 795.000000, reward total was -20.000000. running mean: -20.059929\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.059330\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.068736\n",
            "resetting env. episode 798.000000, reward total was -17.000000. running mean: -20.038049\n",
            "resetting env. episode 799.000000, reward total was -19.000000. running mean: -20.027668\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.037392\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.047018\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.056548\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.065982\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.065322\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.064669\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -20.064022\n",
            "resetting env. episode 807.000000, reward total was -16.000000. running mean: -20.023382\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.033148\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.042817\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.052389\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.051865\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.051346\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.060833\n",
            "resetting env. episode 814.000000, reward total was -18.000000. running mean: -20.040224\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.049822\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.059324\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.068731\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.078043\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.077263\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.076490\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -20.075725\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -20.064968\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -20.064319\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.073675\n",
            "resetting env. episode 825.000000, reward total was -19.000000. running mean: -20.062939\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.062309\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.071686\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.080969\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.090160\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.099258\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -20.088265\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.087383\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.086509\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.085644\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.084787\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.083939\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.093100\n",
            "resetting env. episode 838.000000, reward total was -19.000000. running mean: -20.082169\n",
            "resetting env. episode 839.000000, reward total was -19.000000. running mean: -20.071347\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.080634\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.089828\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -20.088929\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.088040\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.097160\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.106188\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.105126\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.114075\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.122934\n",
            "resetting env. episode 849.000000, reward total was -17.000000. running mean: -20.091705\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.100788\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.109780\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.108682\n",
            "resetting env. episode 853.000000, reward total was -18.000000. running mean: -20.087595\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.096719\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.105752\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.104695\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.103648\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.102611\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.111585\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.120469\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.119265\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.128072\n",
            "resetting env. episode 863.000000, reward total was -19.000000. running mean: -20.116791\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.115623\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.124467\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.133222\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.141890\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.150471\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.158966\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.157377\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.165803\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.174145\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.182404\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.190580\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.198674\n",
            "resetting env. episode 876.000000, reward total was -18.000000. running mean: -20.176687\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.184920\n",
            "resetting env. episode 878.000000, reward total was -17.000000. running mean: -20.153071\n",
            "resetting env. episode 879.000000, reward total was -19.000000. running mean: -20.141540\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.140125\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.138724\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.137336\n",
            "resetting env. episode 883.000000, reward total was -19.000000. running mean: -20.125963\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.124703\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.133456\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -20.132122\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.140801\n",
            "resetting env. episode 888.000000, reward total was -18.000000. running mean: -20.119393\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.128199\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.136917\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.145547\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.154092\n",
            "resetting env. episode 893.000000, reward total was -18.000000. running mean: -20.132551\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.141226\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.129813\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -20.118515\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.127330\n",
            "resetting env. episode 898.000000, reward total was -18.000000. running mean: -20.106057\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.114996\n",
            "resetting env. episode 900.000000, reward total was -19.000000. running mean: -20.103846\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.112808\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.111680\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.110563\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.119457\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.128263\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -20.126980\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.125710\n",
            "resetting env. episode 908.000000, reward total was -19.000000. running mean: -20.114453\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.113309\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.112175\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.111054\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -20.099943\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.088944\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.088054\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.097174\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.096202\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.105240\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.104188\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.113146\n",
            "resetting env. episode 920.000000, reward total was -19.000000. running mean: -20.102014\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.110994\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.119884\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.128685\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.137399\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.146025\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.154564\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -20.133019\n",
            "resetting env. episode 928.000000, reward total was -18.000000. running mean: -20.111688\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.110572\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.119466\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.118271\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.117088\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.125918\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -20.124658\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.123412\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.132178\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.130856\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.139547\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.138152\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.136770\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.135403\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.144049\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.142608\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.151182\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -20.139670\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.148274\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.136791\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.145423\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.143969\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.152529\n",
            "resetting env. episode 951.000000, reward total was -20.000000. running mean: -20.151004\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -20.139494\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -20.138099\n",
            "resetting env. episode 954.000000, reward total was -18.000000. running mean: -20.116718\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.125551\n",
            "resetting env. episode 956.000000, reward total was -19.000000. running mean: -20.114295\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.113152\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.122021\n",
            "resetting env. episode 959.000000, reward total was -19.000000. running mean: -20.110800\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.119692\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -20.108495\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.117411\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.126236\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.134974\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.123624\n",
            "resetting env. episode 966.000000, reward total was -19.000000. running mean: -20.112388\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.111264\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.120152\n",
            "resetting env. episode 969.000000, reward total was -19.000000. running mean: -20.108950\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.117861\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.126682\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.135415\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.144061\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.152620\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.161094\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.169483\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.177788\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.186010\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.194150\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.202209\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.210187\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.218085\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -20.215904\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.203745\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.211708\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.219590\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.227395\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.235121\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.232769\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.240442\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.238037\n",
            "resetting env. episode 992.000000, reward total was -19.000000. running mean: -20.225657\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.223400\n",
            "resetting env. episode 994.000000, reward total was -19.000000. running mean: -20.211166\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.219055\n",
            "resetting env. episode 996.000000, reward total was -19.000000. running mean: -20.206864\n",
            "resetting env. episode 997.000000, reward total was -20.000000. running mean: -20.204796\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -20.192748\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -20.190820\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.198912\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.196923\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.194954\n",
            "resetting env. episode 1003.000000, reward total was -19.000000. running mean: -20.183004\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.171174\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.169462\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.167768\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.166090\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.174429\n",
            "resetting env. episode 1009.000000, reward total was -17.000000. running mean: -20.142685\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.151258\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.159745\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -20.148148\n",
            "resetting env. episode 1013.000000, reward total was -20.000000. running mean: -20.146666\n",
            "resetting env. episode 1014.000000, reward total was -19.000000. running mean: -20.135200\n",
            "resetting env. episode 1015.000000, reward total was -19.000000. running mean: -20.123848\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -20.112609\n",
            "resetting env. episode 1017.000000, reward total was -18.000000. running mean: -20.091483\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.090568\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.089663\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.098766\n",
            "resetting env. episode 1021.000000, reward total was -18.000000. running mean: -20.077778\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.087001\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.096131\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.105169\n",
            "resetting env. episode 1025.000000, reward total was -19.000000. running mean: -20.094118\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.093176\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.092245\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -20.081322\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.090509\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.099604\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.108608\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.107522\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.116447\n",
            "resetting env. episode 1034.000000, reward total was -19.000000. running mean: -20.105282\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.114229\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.123087\n",
            "resetting env. episode 1037.000000, reward total was -18.000000. running mean: -20.101856\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.100838\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.109829\n",
            "resetting env. episode 1040.000000, reward total was -19.000000. running mean: -20.098731\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.087744\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.096866\n",
            "resetting env. episode 1043.000000, reward total was -19.000000. running mean: -20.085897\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.095038\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.094088\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.103147\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.102116\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -20.091095\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.090184\n",
            "resetting env. episode 1050.000000, reward total was -17.000000. running mean: -20.059282\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.068689\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.068002\n",
            "resetting env. episode 1053.000000, reward total was -18.000000. running mean: -20.047322\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -20.036849\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.036480\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.046116\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -20.045654\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.055198\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.054646\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.064099\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.073458\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.082724\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.091897\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.100978\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.109968\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.118868\n",
            "resetting env. episode 1067.000000, reward total was -19.000000. running mean: -20.107680\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.116603\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.115437\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.114282\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.113139\n",
            "resetting env. episode 1072.000000, reward total was -18.000000. running mean: -20.092008\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.101088\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.110077\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.108976\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.117887\n",
            "resetting env. episode 1077.000000, reward total was -19.000000. running mean: -20.106708\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.105641\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -20.104584\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.103538\n",
            "resetting env. episode 1081.000000, reward total was -19.000000. running mean: -20.092503\n",
            "resetting env. episode 1082.000000, reward total was -18.000000. running mean: -20.071578\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.070862\n",
            "resetting env. episode 1084.000000, reward total was -19.000000. running mean: -20.060154\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.059552\n",
            "resetting env. episode 1086.000000, reward total was -19.000000. running mean: -20.048957\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.058467\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.067882\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.077203\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.086431\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.095567\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -20.094611\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.093665\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.092729\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.101801\n",
            "resetting env. episode 1096.000000, reward total was -18.000000. running mean: -20.080783\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.079976\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.089176\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.098284\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.107301\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.106228\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.115166\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -20.104014\n",
            "resetting env. episode 1104.000000, reward total was -17.000000. running mean: -20.072974\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -20.072244\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.081522\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.080707\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.079900\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.079101\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.078310\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.077527\n",
            "resetting env. episode 1112.000000, reward total was -18.000000. running mean: -20.056751\n",
            "resetting env. episode 1113.000000, reward total was -19.000000. running mean: -20.046184\n",
            "resetting env. episode 1114.000000, reward total was -19.000000. running mean: -20.035722\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -20.035365\n",
            "resetting env. episode 1116.000000, reward total was -19.000000. running mean: -20.025011\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.034761\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.044413\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.043969\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.043530\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.053094\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.062563\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -20.061938\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -20.061318\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.060705\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.060098\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -20.059497\n",
            "resetting env. episode 1128.000000, reward total was -18.000000. running mean: -20.038902\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.048513\n",
            "resetting env. episode 1130.000000, reward total was -20.000000. running mean: -20.048028\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.047548\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -20.037072\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.046701\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.046234\n",
            "resetting env. episode 1135.000000, reward total was -18.000000. running mean: -20.025772\n",
            "resetting env. episode 1136.000000, reward total was -17.000000. running mean: -19.995514\n",
            "resetting env. episode 1137.000000, reward total was -18.000000. running mean: -19.975559\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -19.975804\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -19.966046\n",
            "resetting env. episode 1140.000000, reward total was -19.000000. running mean: -19.956385\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -19.956821\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -19.957253\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -19.947681\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -19.958204\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -19.948622\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -19.949135\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -19.949644\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -19.950148\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -19.960646\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -19.961040\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -19.971429\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -19.981715\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -19.991898\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.001979\n",
            "resetting env. episode 1155.000000, reward total was -19.000000. running mean: -19.991959\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -19.992040\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.002119\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.012098\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.021977\n",
            "resetting env. episode 1160.000000, reward total was -15.000000. running mean: -19.971757\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -19.972040\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -19.972319\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -19.972596\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -19.962870\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -19.953241\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -19.953709\n",
            "resetting env. episode 1167.000000, reward total was -20.000000. running mean: -19.954172\n",
            "resetting env. episode 1168.000000, reward total was -19.000000. running mean: -19.944630\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -19.945184\n",
            "resetting env. episode 1170.000000, reward total was -18.000000. running mean: -19.925732\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -19.936475\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.937110\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.947739\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -19.958261\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -19.968679\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -19.978992\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -19.979202\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -19.979410\n",
            "resetting env. episode 1179.000000, reward total was -19.000000. running mean: -19.969616\n",
            "resetting env. episode 1180.000000, reward total was -19.000000. running mean: -19.959920\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -19.960321\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.960717\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -19.961110\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -19.971499\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -19.971784\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -19.972066\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -19.982346\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -19.982522\n",
            "resetting env. episode 1189.000000, reward total was -19.000000. running mean: -19.972697\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -19.982970\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -19.983140\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.993309\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.003376\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.003342\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -20.003309\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.013276\n",
            "resetting env. episode 1197.000000, reward total was -19.000000. running mean: -20.003143\n",
            "resetting env. episode 1198.000000, reward total was -19.000000. running mean: -19.993111\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.003180\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.013148\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.013017\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.022887\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.032658\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.042331\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.051908\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -20.041389\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.030975\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.040665\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.050259\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.059756\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.069159\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -20.068467\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -20.057782\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.057204\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.046632\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.046166\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -20.055704\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.065147\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -20.064496\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.063851\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.073212\n",
            "resetting env. episode 1222.000000, reward total was -20.000000. running mean: -20.072480\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.081756\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.090938\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.100029\n",
            "resetting env. episode 1226.000000, reward total was -20.000000. running mean: -20.099028\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.108038\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.116958\n",
            "resetting env. episode 1229.000000, reward total was -20.000000. running mean: -20.115788\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.114630\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.123484\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.122249\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.131027\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.139716\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -20.128319\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.137036\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.145666\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.144209\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -20.142767\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.141339\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.149926\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.158427\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.156842\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.155274\n",
            "resetting env. episode 1245.000000, reward total was -18.000000. running mean: -20.133721\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.142384\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.150960\n",
            "resetting env. episode 1248.000000, reward total was -18.000000. running mean: -20.129450\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.138156\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -20.126774\n",
            "resetting env. episode 1251.000000, reward total was -19.000000. running mean: -20.115507\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.124352\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.133108\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.131777\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -20.140459\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.149055\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.157564\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.165988\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.174329\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.172585\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.180859\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -20.169051\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -20.167360\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.175687\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.173930\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.182191\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.190369\n",
            "resetting env. episode 1268.000000, reward total was -19.000000. running mean: -20.178465\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.186680\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.184813\n",
            "resetting env. episode 1271.000000, reward total was -19.000000. running mean: -20.172965\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -20.161236\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.159623\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -20.148027\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.146547\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.145081\n",
            "resetting env. episode 1277.000000, reward total was -18.000000. running mean: -20.123631\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.122394\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.121170\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.129959\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -20.118659\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -20.107472\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.116398\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.115234\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.124081\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.132841\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -20.121512\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -20.110297\n",
            "resetting env. episode 1289.000000, reward total was -19.000000. running mean: -20.099194\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.108202\n",
            "resetting env. episode 1291.000000, reward total was -18.000000. running mean: -20.087120\n",
            "resetting env. episode 1292.000000, reward total was -18.000000. running mean: -20.066249\n",
            "resetting env. episode 1293.000000, reward total was -18.000000. running mean: -20.045586\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.055131\n",
            "resetting env. episode 1295.000000, reward total was -18.000000. running mean: -20.034579\n",
            "resetting env. episode 1296.000000, reward total was -19.000000. running mean: -20.024233\n",
            "resetting env. episode 1297.000000, reward total was -19.000000. running mean: -20.013991\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.003851\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -20.003813\n",
            "resetting env. episode 1300.000000, reward total was -18.000000. running mean: -19.983775\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -19.973937\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -19.984197\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -19.974355\n",
            "resetting env. episode 1304.000000, reward total was -19.000000. running mean: -19.964612\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -19.974966\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -19.985216\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -19.985364\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -19.995510\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -19.995555\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -19.985600\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -19.985744\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -19.995886\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.005927\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.015868\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.025709\n",
            "resetting env. episode 1316.000000, reward total was -17.000000. running mean: -19.995452\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -19.985498\n",
            "resetting env. episode 1318.000000, reward total was -19.000000. running mean: -19.975643\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -19.975886\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -19.986128\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -19.986266\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -19.986404\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -19.996540\n",
            "resetting env. episode 1324.000000, reward total was -17.000000. running mean: -19.966574\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -19.976908\n",
            "resetting env. episode 1326.000000, reward total was -17.000000. running mean: -19.947139\n",
            "resetting env. episode 1327.000000, reward total was -19.000000. running mean: -19.937668\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -19.948291\n",
            "resetting env. episode 1329.000000, reward total was -18.000000. running mean: -19.928808\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -19.929520\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -19.940225\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -19.930823\n",
            "resetting env. episode 1333.000000, reward total was -19.000000. running mean: -19.921515\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.932299\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -19.942976\n",
            "resetting env. episode 1336.000000, reward total was -18.000000. running mean: -19.923547\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -19.924311\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -19.935068\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -19.945717\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -19.956260\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -19.956698\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -19.947131\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -19.957659\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -19.968083\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -19.978402\n",
            "resetting env. episode 1346.000000, reward total was -17.000000. running mean: -19.948618\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -19.949132\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -19.959640\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -19.960044\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -19.950444\n",
            "resetting env. episode 1351.000000, reward total was -19.000000. running mean: -19.940939\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -19.941530\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -19.932114\n",
            "resetting env. episode 1354.000000, reward total was -19.000000. running mean: -19.922793\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -19.933565\n",
            "resetting env. episode 1356.000000, reward total was -18.000000. running mean: -19.914230\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -19.915087\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -19.925937\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -19.926677\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.937410\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -19.938036\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -19.928656\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -19.939369\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -19.939976\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -19.940576\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -19.941170\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -19.941758\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -19.952341\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -19.952817\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -19.963289\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -19.973656\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -19.973920\n",
            "resetting env. episode 1373.000000, reward total was -19.000000. running mean: -19.964181\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.964539\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -19.964893\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -19.965245\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -19.975592\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -19.965836\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -19.966178\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -19.956516\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -19.956951\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -19.967381\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -19.967708\n",
            "resetting env. episode 1384.000000, reward total was -17.000000. running mean: -19.938030\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.948650\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.959164\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -19.969572\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -19.979876\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -19.990078\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -19.990177\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.000275\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.010272\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.020170\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.029968\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.039668\n",
            "resetting env. episode 1396.000000, reward total was -20.000000. running mean: -20.039271\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.048879\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -20.038390\n",
            "resetting env. episode 1399.000000, reward total was -18.000000. running mean: -20.018006\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.017826\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.017648\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -20.017471\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -20.017297\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.027124\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -20.026852\n",
            "resetting env. episode 1406.000000, reward total was -19.000000. running mean: -20.016584\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.026418\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.026154\n",
            "resetting env. episode 1409.000000, reward total was -19.000000. running mean: -20.015892\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.025733\n",
            "resetting env. episode 1411.000000, reward total was -19.000000. running mean: -20.015476\n",
            "resetting env. episode 1412.000000, reward total was -18.000000. running mean: -19.995321\n",
            "resetting env. episode 1413.000000, reward total was -19.000000. running mean: -19.985368\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -19.975514\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.975759\n",
            "resetting env. episode 1416.000000, reward total was -18.000000. running mean: -19.956002\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.956442\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -19.966877\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -19.977208\n",
            "resetting env. episode 1420.000000, reward total was -19.000000. running mean: -19.967436\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -19.977762\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -19.987984\n",
            "resetting env. episode 1423.000000, reward total was -18.000000. running mean: -19.968105\n",
            "resetting env. episode 1424.000000, reward total was -20.000000. running mean: -19.968423\n",
            "resetting env. episode 1425.000000, reward total was -20.000000. running mean: -19.968739\n",
            "resetting env. episode 1426.000000, reward total was -18.000000. running mean: -19.949052\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.959561\n",
            "resetting env. episode 1428.000000, reward total was -18.000000. running mean: -19.939966\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -19.950566\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.951060\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -19.951550\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -19.952034\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -19.962514\n",
            "resetting env. episode 1434.000000, reward total was -19.000000. running mean: -19.952889\n",
            "resetting env. episode 1435.000000, reward total was -18.000000. running mean: -19.933360\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -19.944026\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -19.934586\n",
            "resetting env. episode 1438.000000, reward total was -20.000000. running mean: -19.935240\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -19.925888\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -19.926629\n",
            "resetting env. episode 1441.000000, reward total was -18.000000. running mean: -19.907363\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -19.918289\n",
            "resetting env. episode 1443.000000, reward total was -19.000000. running mean: -19.909106\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -19.920015\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -19.920815\n",
            "resetting env. episode 1446.000000, reward total was -19.000000. running mean: -19.911607\n",
            "resetting env. episode 1447.000000, reward total was -18.000000. running mean: -19.892491\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -19.893566\n",
            "resetting env. episode 1449.000000, reward total was -19.000000. running mean: -19.884630\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -19.875784\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -19.887026\n",
            "resetting env. episode 1452.000000, reward total was -19.000000. running mean: -19.878156\n",
            "resetting env. episode 1453.000000, reward total was -17.000000. running mean: -19.849374\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -19.860880\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.872272\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -19.883549\n",
            "resetting env. episode 1457.000000, reward total was -19.000000. running mean: -19.874713\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.875966\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -19.887207\n",
            "resetting env. episode 1460.000000, reward total was -18.000000. running mean: -19.868335\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -19.869651\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -19.880955\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -19.882145\n",
            "resetting env. episode 1464.000000, reward total was -18.000000. running mean: -19.863324\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -19.864690\n",
            "resetting env. episode 1466.000000, reward total was -19.000000. running mean: -19.856044\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -19.867483\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -19.868808\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -19.880120\n",
            "resetting env. episode 1470.000000, reward total was -18.000000. running mean: -19.861319\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -19.872706\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -19.873979\n",
            "resetting env. episode 1473.000000, reward total was -18.000000. running mean: -19.855239\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.866687\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -19.868020\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -19.869340\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -19.870646\n",
            "resetting env. episode 1478.000000, reward total was -20.000000. running mean: -19.871940\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -19.863220\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -19.874588\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -19.885842\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -19.896984\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -19.908014\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -19.918934\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -19.919744\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -19.930547\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -19.931242\n",
            "resetting env. episode 1488.000000, reward total was -19.000000. running mean: -19.921929\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -19.922710\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -19.923483\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.934248\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -19.944905\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -19.945456\n",
            "resetting env. episode 1494.000000, reward total was -19.000000. running mean: -19.936002\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.936642\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -19.947275\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -19.957803\n",
            "resetting env. episode 1498.000000, reward total was -19.000000. running mean: -19.948225\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -19.958742\n",
            "resetting env. episode 1500.000000, reward total was -18.000000. running mean: -19.939155\n",
            "CPU times: user 1h 52min 5s, sys: 35min 12s, total: 2h 27min 18s\n",
            "Wall time: 1h 16min 2s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "8f79d4ba-73c8-4683-d52b-65fcaa791470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}