{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "dbe073b4-55ef-4593-83df-aab2ca1dcd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=8f887cb2b92fbff91b0931971a405355d79fb844a2cb5e31a7c7cf2061db8da6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "2916095c-e39d-4689-aaea-b7a8af636423"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "935f09ca-5959-424c-f004-f39797801a97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "39117354-88e2-4bc6-e897-9b9a97d4de66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "bcfeb07b-dd01-4d17-c81d-44f1a1491e5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "2cc535b2-862b-48da-aa60-98b700729491"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 10.000000, reward total was -17.000000. running mean: -20.940882\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.941473\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.942059\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.932638\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.933312\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.923979\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.914739\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.915591\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.906435\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.897371\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.898397\n",
            "resetting env. episode 21.000000, reward total was -18.000000. running mean: -20.869413\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.860719\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.852112\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.853591\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.855055\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.856505\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.857939\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.859360\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.850766\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.852259\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.853736\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.845199\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.836747\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.838379\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.829996\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.831696\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.833379\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.825045\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.826794\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.808527\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.810441\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.812337\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.814213\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.816071\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.797911\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.799932\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.801932\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.793913\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.795974\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.778014\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.770234\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.772532\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.764806\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.747158\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.729687\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.732390\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.735066\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.737715\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.720338\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.723135\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.725903\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.728644\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.711358\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.704244\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.707202\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.710130\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.713028\n",
            "resetting env. episode 68.000000, reward total was -17.000000. running mean: -20.675898\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.669139\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.672448\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.665723\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.669066\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.672375\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.675652\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.668895\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.672206\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.665484\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.668829\n",
            "resetting env. episode 79.000000, reward total was -18.000000. running mean: -20.642141\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.645720\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.649262\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.652770\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.656242\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.659680\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.653083\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.646552\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.650087\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.653586\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.657050\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.640479\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.634075\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.637734\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.641356\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.644943\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.628493\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.622209\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.615986\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.619827\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.623628\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.627392\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.631118\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.634807\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.638459\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.632074\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.615754\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.619596\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.603400\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.597366\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.601392\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -20.575378\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.579625\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.573828\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.568090\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.562409\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -20.546785\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.541317\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.545904\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.550445\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.554941\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.549391\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.543897\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.538458\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.533074\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.537743\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.542366\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.546942\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.551472\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.545958\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.540498\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.545093\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.549642\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.554146\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.548604\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.553118\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.547587\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.552111\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.546590\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.541124\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.545713\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.540256\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.534853\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.529505\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.514210\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.519068\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.523877\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.528638\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.523352\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.528118\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.532837\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.537509\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.532134\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.536812\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.541444\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.546030\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.540569\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.545164\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.549712\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.544215\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.538773\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.533385\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.528051\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.532771\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.537443\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.522069\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -20.496848\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.501879\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.496861\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.481892\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.477073\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.462302\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.457679\n",
            "resetting env. episode 172.000000, reward total was -17.000000. running mean: -20.423103\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.408872\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.414783\n",
            "resetting env. episode 175.000000, reward total was -18.000000. running mean: -20.390635\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.376729\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.382961\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.389132\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.395240\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.381288\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.387475\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.373600\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.359864\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.356266\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.352703\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.359176\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.365584\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.361928\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.358309\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.364726\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.361079\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.357468\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.343893\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.350454\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.346950\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.353480\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.359946\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.366346\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.372683\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.378956\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.385166\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.381315\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.387501\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.393626\n",
            "resetting env. episode 205.000000, reward total was -18.000000. running mean: -20.369690\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.375993\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.382233\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.368411\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.374727\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.370980\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.357270\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.363697\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.360060\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.356460\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.362895\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.369266\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.375573\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.371818\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.378099\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.384318\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.390475\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.396571\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.392605\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.388679\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.384792\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.380944\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.387135\n",
            "resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.363263\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.369631\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.375934\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.382175\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.388353\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.384470\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.380625\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.376819\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.373051\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.379320\n",
            "resetting env. episode 238.000000, reward total was -17.000000. running mean: -20.345527\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.352072\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.348551\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.345065\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.341615\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.348199\n",
            "resetting env. episode 244.000000, reward total was -18.000000. running mean: -20.324717\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.321469\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.328255\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.334972\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.341622\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.338206\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.344824\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.351376\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.357862\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.364284\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.360641\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.367034\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.363364\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.369730\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.356033\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.362473\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.358848\n",
            "resetting env. episode 261.000000, reward total was -18.000000. running mean: -20.335259\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.341907\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.348488\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.345003\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.321553\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.328337\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.335054\n",
            "resetting env. episode 268.000000, reward total was -18.000000. running mean: -20.311703\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.318586\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.325401\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.322147\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.328925\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.315636\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.312479\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.309355\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.306261\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.303199\n",
            "resetting env. episode 278.000000, reward total was -18.000000. running mean: -20.280167\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.287365\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.284491\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.291646\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.278730\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.285943\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.283083\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.280252\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.277450\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.274675\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.281929\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.289109\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.296218\n",
            "resetting env. episode 291.000000, reward total was -18.000000. running mean: -20.273256\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.280523\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.287718\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.294841\n",
            "resetting env. episode 295.000000, reward total was -18.000000. running mean: -20.271893\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.279174\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.276382\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.273618\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.280882\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.288073\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.285192\n",
            "resetting env. episode 302.000000, reward total was -18.000000. running mean: -20.262340\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.259717\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.267120\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.274449\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.271704\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.278987\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.276197\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.273435\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.270701\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.277994\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.265214\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.272562\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.279836\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.277038\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.284268\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.281425\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.288611\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.295724\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.302767\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.299740\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.306742\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.293675\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.300738\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.287731\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.294853\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.301905\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.298886\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.305897\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.312838\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.319710\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.316512\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.323347\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.320114\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.326913\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.323644\n",
            "resetting env. episode 337.000000, reward total was -18.000000. running mean: -20.300407\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.307403\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.314329\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.321186\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.317974\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.314794\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.321646\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.328430\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.335145\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.331794\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.338476\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.335091\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.341740\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.348323\n",
            "resetting env. episode 351.000000, reward total was -18.000000. running mean: -20.324840\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.321591\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.328375\n",
            "resetting env. episode 354.000000, reward total was -18.000000. running mean: -20.305092\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.312041\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.318920\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.325731\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.322474\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.329249\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.335957\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.342597\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.339171\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.335779\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.342422\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.348997\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.345507\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.342052\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.338632\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.325245\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.331993\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.338673\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.345286\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.351833\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.358315\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.354732\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.351185\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.347673\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.344196\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.350754\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.347247\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.343774\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.350336\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.356833\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.353265\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.359732\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.366135\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.362473\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.358849\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.365260\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.361608\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.367991\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.374312\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.370568\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.376863\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.383094\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.389263\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.395371\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.391417\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.397503\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.403528\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.399492\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.395497\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.381542\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.387727\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.393850\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.389911\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.396012\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.392052\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.388132\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.394250\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.380308\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.386505\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.392640\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.378713\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.384926\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.391077\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.397166\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.403194\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.409162\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.395071\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.401120\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.397109\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.403138\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.409106\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.415015\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.420865\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.416657\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.412490\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.418365\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.424181\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.429940\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.435640\n",
            "resetting env. episode 433.000000, reward total was -18.000000. running mean: -20.411284\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.407171\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.413099\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.408968\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.404879\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.410830\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.416722\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.402554\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.408529\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.414443\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.420299\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.426096\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.431835\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.437517\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.443142\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.438710\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.424323\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.430080\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.425779\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.431521\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.437206\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.442834\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.448406\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.443922\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.439482\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.425088\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.420837\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.426628\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.422362\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.418138\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.413957\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.409817\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.415719\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.401562\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.397546\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.403571\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.409535\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.415440\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.421286\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.417073\n",
            "resetting env. episode 473.000000, reward total was -18.000000. running mean: -20.392902\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.388973\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.395083\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.391132\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.397221\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.393249\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.379316\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.385523\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.391668\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.387751\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.393874\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.399935\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.395936\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.391976\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.398057\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.404076\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.410035\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.415935\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.421776\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.427558\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.423282\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.429049\n",
            "resetting env. episode 495.000000, reward total was -18.000000. running mean: -20.404759\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.410711\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.396604\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.392638\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.398712\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.384725\n",
            "CPU times: user 2h 1min 58s, sys: 18min 11s, total: 2h 20min 9s\n",
            "Wall time: 1h 13min 9s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "b1acf472-ddd7-4fff-a094-0ae4209e38f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.960398\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.950794\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.941286\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.941873\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.942454\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.943030\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.933600\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.934264\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.924921\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.915672\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.916515\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.907350\n",
            "resetting env. episode 19.000000, reward total was -17.000000. running mean: -20.868276\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.849594\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.841098\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.842687\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.834260\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.835917\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.837558\n",
            "resetting env. episode 26.000000, reward total was -17.000000. running mean: -20.799183\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.791191\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.783279\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.785446\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.777592\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.779816\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.772017\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.774297\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.776554\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.768789\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.771101\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.773390\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.765656\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.757999\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.750419\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.752915\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.745386\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.747932\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.750453\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.752948\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.755419\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.757865\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.760286\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.762683\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.765056\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.767406\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.769732\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.762034\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.764414\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.766770\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -20.749102\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.741611\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.744195\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.746753\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.749286\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.751793\n",
            "resetting env. episode 62.000000, reward total was -19.000000. running mean: -20.734275\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.726932\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.709663\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.692566\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.695640\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.688684\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.691797\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.694879\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.697930\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.700951\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.693942\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.687002\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.670132\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.673431\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.666697\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.660030\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.653429\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.646895\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.640426\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.634022\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.637682\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.641305\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.644892\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.638443\n",
            "resetting env. episode 86.000000, reward total was -18.000000. running mean: -20.612058\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.605938\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.609878\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.613780\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.617642\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.611465\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.615351\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.619197\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.613005\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.616875\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.620706\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.614499\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.618354\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.612171\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.616049\n",
            "resetting env. episode 101.000000, reward total was -18.000000. running mean: -20.589889\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.583990\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.568150\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.572468\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.576744\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.580976\n",
            "resetting env. episode 107.000000, reward total was -18.000000. running mean: -20.555167\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.559615\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.564019\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.548379\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.552895\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.547366\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.541892\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.546473\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.551008\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.555498\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.549943\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.544444\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.549000\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.533510\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.528174\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.522893\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.527664\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.522387\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.527163\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.521892\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.526673\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.521406\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.526192\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.520930\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.525721\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.520463\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.515259\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.520106\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.524905\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.529656\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.534360\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.529016\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.523726\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.518489\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.523304\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.528071\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.532790\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.517462\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.512287\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.497165\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.492193\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.487271\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.482398\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.487574\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.482699\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.487872\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.482993\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.478163\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.473381\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.458647\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.464061\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -20.449420\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.454926\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.450377\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.455873\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.451314\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.456801\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.462233\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.467611\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.472935\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.458205\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.443623\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.449187\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.454695\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.450148\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.455647\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.461090\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.456480\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.451915\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.457396\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.462822\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.468193\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -20.443511\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.439076\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.434686\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.440339\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.435935\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.421576\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.407360\n",
            "resetting env. episode 186.000000, reward total was -17.000000. running mean: -20.373287\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.359554\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.365958\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.362299\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.368676\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.374989\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.381239\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.377427\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.363652\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.360016\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.356416\n",
            "resetting env. episode 197.000000, reward total was -19.000000. running mean: -20.342852\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.349423\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.355929\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.352369\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.358846\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.355257\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.361705\n",
            "resetting env. episode 204.000000, reward total was -18.000000. running mean: -20.338088\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -20.324707\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.331460\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.338145\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.344764\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.351316\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.357803\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.344225\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.350783\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.347275\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.353802\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.350264\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.346761\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.353294\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.359761\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.346163\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.352702\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.349175\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.355683\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.362126\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.368505\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.374820\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.381072\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.387261\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.393388\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.379454\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.385660\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.381803\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.377985\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.374205\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.380463\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.386659\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.392792\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.398864\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.394875\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.400927\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.396917\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.402948\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.408919\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.414830\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.420681\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.426474\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.432210\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.437888\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.423509\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.429274\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.434981\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.420631\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.426425\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.422161\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.427939\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.433660\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.429323\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.435030\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.440679\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.426273\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.432010\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.437690\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.433313\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.438980\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.444590\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.450144\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.455643\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.461086\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.456475\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.461911\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.467292\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.472619\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.467892\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.443213\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.448781\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.454294\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.459751\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.465153\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.460502\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.455897\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.451338\n",
            "resetting env. episode 281.000000, reward total was -18.000000. running mean: -20.426824\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.422556\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.418330\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.424147\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.419906\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.425707\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.421450\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.427235\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.432963\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.428633\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.424347\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.420103\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.415902\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.401743\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.407726\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.413648\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.389512\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.385617\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.391761\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.397843\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.393865\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.399926\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.385927\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.392068\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.398147\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.384165\n",
            "resetting env. episode 307.000000, reward total was -18.000000. running mean: -20.360324\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.366720\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.353053\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.359523\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.365928\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.372268\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.378546\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.384760\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.390912\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.377003\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.373233\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.379501\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.385706\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.391849\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.397930\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.393951\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.390012\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.396112\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.402150\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.408129\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.394048\n",
            "resetting env. episode 328.000000, reward total was -16.000000. running mean: -20.350107\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.356606\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.363040\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.369410\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.375716\n",
            "resetting env. episode 333.000000, reward total was -18.000000. running mean: -20.351958\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.358439\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.364854\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.371206\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.377494\n",
            "resetting env. episode 338.000000, reward total was -18.000000. running mean: -20.353719\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.340182\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.336780\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.343412\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.349978\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.356478\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.362913\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.369284\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.345591\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.332135\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.338814\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.345426\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.341972\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.348552\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.355066\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.361516\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.367901\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.374222\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.380479\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.376675\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.372908\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.379179\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.385387\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.391533\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.387618\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.393742\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.399804\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.405806\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.411748\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.407631\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.403554\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.389519\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.395624\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.401667\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.387651\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.383774\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.389936\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.396037\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.382077\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.388256\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.394373\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.390430\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.386525\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.392660\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.398733\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.404746\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.390699\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.396792\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.402824\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.408796\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.414708\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.420561\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.416355\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.422191\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.427969\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.433690\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.419353\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.425159\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.430908\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.416599\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.412433\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.418308\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.404125\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.390084\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.376183\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.362421\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.368797\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.365109\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.371458\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.377743\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.383966\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.380126\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.386325\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.372462\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.378737\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.384950\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.381100\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.387289\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.373416\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.379682\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.385885\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.382027\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.378206\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.384424\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.370580\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.366874\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.373206\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.359473\n",
            "resetting env. episode 426.000000, reward total was -15.000000. running mean: -20.305879\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.302820\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.309792\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.306694\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.303627\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.300591\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.307585\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.314509\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.311364\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.308250\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.295168\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.302216\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.299194\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.296202\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.303240\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.310207\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.307105\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.314034\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.300894\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.307885\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.314806\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.321658\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.328442\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.315157\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.322006\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.328786\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.325498\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.322243\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.319020\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.325830\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.332572\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.339246\n",
            "resetting env. episode 458.000000, reward total was -18.000000. running mean: -20.315854\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.322695\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.319468\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.316273\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.323111\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.329880\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.326581\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.313315\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.310182\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.317080\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.313909\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.320770\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.327562\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.324287\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.321044\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.317833\n",
            "resetting env. episode 474.000000, reward total was -16.000000. running mean: -20.274655\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.271909\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.259189\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.266598\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.273932\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.261192\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.258580\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.265995\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.273335\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.280601\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.277795\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.275017\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.282267\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.289444\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.286550\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.293685\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.290748\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.297840\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.294862\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.301913\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.298894\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.295905\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.302946\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.309917\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.306817\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.303749\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.310712\n",
            "CPU times: user 2h 1min 56s, sys: 18min 16s, total: 2h 20min 12s\n",
            "Wall time: 1h 12min 56s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "be14f0a8-1b8c-47ee-eea2-95774f37a6ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AxOcQhIsKow",
        "outputId": "b7c386cd-a719-4008-abcf-50c202c62ddb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.960299\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.960696\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.961089\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.961478\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.961863\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.962245\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.962622\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.952996\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.943466\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.934031\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.924691\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.925444\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.926190\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.906928\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.907859\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.898780\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.889792\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.890894\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.871985\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.863266\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.864633\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.865987\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.847327\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.848853\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.840365\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.841961\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.843542\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.825106\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.826855\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.828587\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.830301\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.811998\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.803878\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.785839\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.787981\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.780101\n",
            "resetting env. episode 41.000000, reward total was -19.000000. running mean: -20.762300\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.754677\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.757130\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.759559\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.751963\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.754443\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.756899\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.749330\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.741837\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.734418\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.737074\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.739703\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.722306\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.725083\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.707833\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.710754\n",
            "resetting env. episode 57.000000, reward total was -18.000000. running mean: -20.683647\n",
            "resetting env. episode 58.000000, reward total was -18.000000. running mean: -20.656810\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.640242\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -20.623840\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.627601\n",
            "resetting env. episode 62.000000, reward total was -17.000000. running mean: -20.591325\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.595412\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.589458\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.593563\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.587628\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.591751\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.595834\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.599876\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.603877\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.597838\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.601860\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.605841\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.599783\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.603785\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.597747\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.581769\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.585952\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.590092\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.594191\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.588249\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.592367\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.596443\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.580479\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.574674\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.568927\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.563238\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.547606\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.542130\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.536708\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.531341\n",
            "resetting env. episode 92.000000, reward total was -17.000000. running mean: -20.496028\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.481068\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.486257\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.491394\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.496480\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.481516\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.486700\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.481833\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.487015\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.482145\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.487323\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.482450\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.477626\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.482849\n",
            "resetting env. episode 106.000000, reward total was -18.000000. running mean: -20.458021\n",
            "resetting env. episode 107.000000, reward total was -18.000000. running mean: -20.433441\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.429106\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.434815\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -20.410467\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.416362\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.422199\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.427977\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.433697\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.439360\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.444966\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.450517\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.456012\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.461452\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.456837\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.462269\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.467646\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.462970\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.468340\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.463656\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.469020\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.464330\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.469686\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.464989\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.460340\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -20.445736\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.441279\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.446866\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.442397\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.447973\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.443494\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.449059\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.444568\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.450122\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.455621\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.461065\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.466454\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.461790\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.457172\n",
            "resetting env. episode 145.000000, reward total was -18.000000. running mean: -20.432600\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.428274\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.423991\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.419752\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.425554\n",
            "resetting env. episode 150.000000, reward total was -18.000000. running mean: -20.401299\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.407286\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.413213\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.419081\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.414890\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.420741\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.426533\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.412268\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.418145\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.413964\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.399824\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.395826\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.401868\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.397849\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.403871\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.389832\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.395934\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.401974\n",
            "resetting env. episode 168.000000, reward total was -18.000000. running mean: -20.377955\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.374175\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.370433\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.356729\n",
            "resetting env. episode 172.000000, reward total was -17.000000. running mean: -20.323162\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.329930\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.326631\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.323364\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.320131\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.326929\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.333660\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.340324\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.336920\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.343551\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.330116\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.326814\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.333546\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.340211\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.346809\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.343341\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.339907\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.346508\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.353043\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.349513\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.346018\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.352557\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.349032\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.345541\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.342086\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.348665\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.355179\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.351627\n",
            "resetting env. episode 200.000000, reward total was -18.000000. running mean: -20.328110\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.324829\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.311581\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.308465\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.305381\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.312327\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.319204\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.316012\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.302851\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.299823\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.296825\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.293856\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.290918\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.278009\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.285229\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.282376\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.269553\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.256857\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.254288\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.261746\n",
            "resetting env. episode 220.000000, reward total was -17.000000. running mean: -20.229128\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.226837\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.234568\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.242223\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.249801\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.257303\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.264730\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.272082\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.279361\n",
            "resetting env. episode 229.000000, reward total was -18.000000. running mean: -20.256568\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.264002\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.271362\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.278648\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.285862\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.293003\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.300073\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.297073\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.294102\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.291161\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.298249\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.295267\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.282314\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.289491\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.296596\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.293630\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.300694\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.287687\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.294810\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.301862\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.298843\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.305855\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.312796\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.319668\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.306472\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.313407\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.320273\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.317070\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.313899\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.300760\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.287753\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.294875\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.301927\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.288907\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.296018\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.303058\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.310027\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.296927\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.303958\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.310918\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.297809\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.294831\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.291883\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.288964\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.296074\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.303114\n",
            "resetting env. episode 275.000000, reward total was -18.000000. running mean: -20.280082\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.287282\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.294409\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.291465\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.288550\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.275664\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.272908\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.280179\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.277377\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.274603\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.281857\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.269039\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.266348\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.273685\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.270948\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.268238\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.265556\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.252900\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.240371\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.237968\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.245588\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.233132\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.220801\n",
            "resetting env. episode 298.000000, reward total was -19.000000. running mean: -20.208593\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.206507\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.204442\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.212397\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.220273\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.228071\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.215790\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.223632\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.211396\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.219282\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.227089\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.234818\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.242470\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.240045\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.247645\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.255168\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.262617\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.269991\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.267291\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.274618\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.271872\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.279153\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.276361\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.263598\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.270962\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.278252\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.285470\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.292615\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.279689\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.276892\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.284123\n",
            "resetting env. episode 329.000000, reward total was -17.000000. running mean: -20.251282\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.258769\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.256181\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.253619\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.261083\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.248472\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.255988\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.253428\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.240893\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.238485\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.236100\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.233739\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.241401\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.248987\n",
            "resetting env. episode 343.000000, reward total was -18.000000. running mean: -20.226497\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.234232\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.231890\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.239571\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.247175\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.244704\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.242257\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.229834\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.237536\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.235160\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.222809\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.230581\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.228275\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.225992\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.223732\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.221495\n",
            "resetting env. episode 359.000000, reward total was -18.000000. running mean: -20.199280\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.207287\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.195214\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.203262\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.201230\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.199217\n",
            "resetting env. episode 365.000000, reward total was -17.000000. running mean: -20.167225\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.175553\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.183797\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.181959\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.190140\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.188238\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.196356\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.184392\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.192548\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.180623\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.178817\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.187029\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.195158\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.203207\n",
            "resetting env. episode 379.000000, reward total was -18.000000. running mean: -20.181175\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.189363\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.187469\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.175595\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.183839\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.192000\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.200080\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.208079\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.215999\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.223839\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.231600\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.219284\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.227091\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.224821\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.222572\n",
            "resetting env. episode 394.000000, reward total was -18.000000. running mean: -20.200347\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.208343\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.196260\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -20.174297\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.182554\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.190729\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.198821\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.186833\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.194965\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.193015\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.201085\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.189074\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.197183\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.195212\n",
            "resetting env. episode 408.000000, reward total was -18.000000. running mean: -20.173259\n",
            "resetting env. episode 409.000000, reward total was -18.000000. running mean: -20.151527\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.160012\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.168411\n",
            "resetting env. episode 412.000000, reward total was -17.000000. running mean: -20.136727\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.145360\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.143906\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.152467\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.150943\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.149433\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.157939\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.156360\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.164796\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.173148\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.181417\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.189602\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.187706\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.175829\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.184071\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.192230\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.200308\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.188305\n",
            "resetting env. episode 430.000000, reward total was -17.000000. running mean: -20.156422\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.164858\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.163209\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.161577\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.149961\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.158462\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.166877\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.175208\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.183456\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.171622\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.179905\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.188106\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.176225\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.184463\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.172618\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.180892\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.189083\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.177192\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.185420\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.193566\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.201631\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.209614\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.217518\n",
            "resetting env. episode 453.000000, reward total was -18.000000. running mean: -20.195343\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.173390\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.181656\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.179839\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.168041\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.156360\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.164797\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.173149\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.161417\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.169803\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.178105\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.176324\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.184561\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.192715\n",
            "resetting env. episode 467.000000, reward total was -17.000000. running mean: -20.160788\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.149180\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -20.127688\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.126411\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.125147\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.123896\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.132657\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.141330\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.139917\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.148518\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.137033\n",
            "resetting env. episode 478.000000, reward total was -18.000000. running mean: -20.115662\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.114506\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.103361\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.112327\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.121204\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.129992\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.118692\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.127505\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.126230\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.124968\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.133718\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.142381\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.140957\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.149547\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.138052\n",
            "resetting env. episode 493.000000, reward total was -18.000000. running mean: -20.116671\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.115505\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.124350\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.133106\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.141775\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.150357\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.158854\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.147265\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.145793\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.154335\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.152791\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.161263\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.159651\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.158054\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.166474\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.164809\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.173161\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.171429\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.169715\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.178018\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.186238\n",
            "resetting env. episode 514.000000, reward total was -18.000000. running mean: -20.164375\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.172731\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.181004\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.189194\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.187302\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.195429\n",
            "resetting env. episode 520.000000, reward total was -18.000000. running mean: -20.173475\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.171740\n",
            "resetting env. episode 522.000000, reward total was -18.000000. running mean: -20.150023\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.138522\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.147137\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.145666\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.154209\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.152667\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.161140\n",
            "resetting env. episode 529.000000, reward total was -19.000000. running mean: -20.149529\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.148034\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.156553\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.154988\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.163438\n",
            "resetting env. episode 534.000000, reward total was -13.000000. running mean: -20.091804\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.090886\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.089977\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.099077\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.108086\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.107005\n",
            "resetting env. episode 540.000000, reward total was -20.000000. running mean: -20.105935\n",
            "resetting env. episode 541.000000, reward total was -18.000000. running mean: -20.084876\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.094027\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.103087\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.102056\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.101035\n",
            "resetting env. episode 546.000000, reward total was -18.000000. running mean: -20.080025\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.079225\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.088433\n",
            "resetting env. episode 549.000000, reward total was -19.000000. running mean: -20.077548\n",
            "resetting env. episode 550.000000, reward total was -18.000000. running mean: -20.056773\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.066205\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.075543\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.074788\n",
            "resetting env. episode 554.000000, reward total was -18.000000. running mean: -20.054040\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.063499\n",
            "resetting env. episode 556.000000, reward total was -21.000000. running mean: -20.072864\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.072136\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.081414\n",
            "resetting env. episode 559.000000, reward total was -18.000000. running mean: -20.060600\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.069994\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.079294\n",
            "resetting env. episode 562.000000, reward total was -19.000000. running mean: -20.068501\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -20.067816\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.077138\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.086367\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.085503\n",
            "resetting env. episode 567.000000, reward total was -19.000000. running mean: -20.074648\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.083902\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.073063\n",
            "resetting env. episode 570.000000, reward total was -19.000000. running mean: -20.062332\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.071709\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.070992\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.070282\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -20.059579\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.068983\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.078293\n",
            "resetting env. episode 577.000000, reward total was -19.000000. running mean: -20.067510\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.076835\n",
            "resetting env. episode 579.000000, reward total was -17.000000. running mean: -20.046067\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -20.035606\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.025250\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.024998\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.024748\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.034500\n",
            "resetting env. episode 585.000000, reward total was -19.000000. running mean: -20.024155\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.033914\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.043574\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.053139\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.052607\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.052081\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.061560\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.060945\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.070335\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.079632\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.088836\n",
            "resetting env. episode 596.000000, reward total was -18.000000. running mean: -20.067947\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.077268\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.086495\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.095630\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.104674\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.113627\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.102491\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.101466\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.110451\n",
            "resetting env. episode 605.000000, reward total was -19.000000. running mean: -20.099347\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.098353\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.107370\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.116296\n",
            "resetting env. episode 609.000000, reward total was -18.000000. running mean: -20.095133\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.094182\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.103240\n",
            "resetting env. episode 612.000000, reward total was -20.000000. running mean: -20.102208\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.111186\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.120074\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.128873\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.117584\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.126408\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.135144\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.143793\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.142355\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.150931\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.159422\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.167828\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.176150\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.174388\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.182644\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.180818\n",
            "resetting env. episode 628.000000, reward total was -18.000000. running mean: -20.159010\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.147419\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.155945\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.154386\n",
            "resetting env. episode 632.000000, reward total was -18.000000. running mean: -20.132842\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.141514\n",
            "resetting env. episode 634.000000, reward total was -17.000000. running mean: -20.110098\n",
            "resetting env. episode 635.000000, reward total was -17.000000. running mean: -20.078997\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.078207\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.077425\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.086651\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.095785\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.094827\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.103878\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.102840\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.101811\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.100793\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.099785\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.098787\n",
            "resetting env. episode 647.000000, reward total was -20.000000. running mean: -20.097800\n",
            "resetting env. episode 648.000000, reward total was -18.000000. running mean: -20.076822\n",
            "resetting env. episode 649.000000, reward total was -18.000000. running mean: -20.056053\n",
            "resetting env. episode 650.000000, reward total was -20.000000. running mean: -20.055493\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.064938\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.074288\n",
            "resetting env. episode 653.000000, reward total was -18.000000. running mean: -20.053546\n",
            "resetting env. episode 654.000000, reward total was -17.000000. running mean: -20.023010\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.032780\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -20.022452\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.022228\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.022005\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.031785\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.041468\n",
            "resetting env. episode 661.000000, reward total was -19.000000. running mean: -20.031053\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.030742\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.040435\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.050031\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.049530\n",
            "resetting env. episode 666.000000, reward total was -19.000000. running mean: -20.039035\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.048645\n",
            "resetting env. episode 668.000000, reward total was -19.000000. running mean: -20.038158\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.037777\n",
            "resetting env. episode 670.000000, reward total was -20.000000. running mean: -20.037399\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.027025\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.036755\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.046387\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.055923\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.055364\n",
            "resetting env. episode 676.000000, reward total was -19.000000. running mean: -20.044810\n",
            "resetting env. episode 677.000000, reward total was -18.000000. running mean: -20.024362\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -20.014119\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.013977\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.023838\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.033599\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.043263\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.052831\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.062302\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.051679\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.061162\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.070551\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -20.059845\n",
            "resetting env. episode 689.000000, reward total was -20.000000. running mean: -20.059247\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.068654\n",
            "resetting env. episode 691.000000, reward total was -18.000000. running mean: -20.047968\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.047488\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.057013\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.056443\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.065879\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.075220\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.084468\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.093623\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.102687\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.111660\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -20.110543\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.119438\n",
            "resetting env. episode 703.000000, reward total was -19.000000. running mean: -20.108244\n",
            "resetting env. episode 704.000000, reward total was -18.000000. running mean: -20.087161\n",
            "resetting env. episode 705.000000, reward total was -17.000000. running mean: -20.056290\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.065727\n",
            "resetting env. episode 707.000000, reward total was -18.000000. running mean: -20.045069\n",
            "resetting env. episode 708.000000, reward total was -19.000000. running mean: -20.034619\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.044272\n",
            "resetting env. episode 710.000000, reward total was -18.000000. running mean: -20.023830\n",
            "resetting env. episode 711.000000, reward total was -17.000000. running mean: -19.993591\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -19.993656\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -19.993719\n",
            "resetting env. episode 714.000000, reward total was -19.000000. running mean: -19.983782\n",
            "resetting env. episode 715.000000, reward total was -19.000000. running mean: -19.973944\n",
            "resetting env. episode 716.000000, reward total was -17.000000. running mean: -19.944205\n",
            "resetting env. episode 717.000000, reward total was -18.000000. running mean: -19.924762\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -19.925515\n",
            "resetting env. episode 719.000000, reward total was -19.000000. running mean: -19.916260\n",
            "resetting env. episode 720.000000, reward total was -19.000000. running mean: -19.907097\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -19.908026\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -19.918946\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -19.929756\n",
            "resetting env. episode 724.000000, reward total was -18.000000. running mean: -19.910459\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -19.911354\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -19.912241\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -19.923118\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -19.933887\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -19.944548\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -19.955103\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -19.965552\n",
            "resetting env. episode 732.000000, reward total was -19.000000. running mean: -19.955896\n",
            "resetting env. episode 733.000000, reward total was -18.000000. running mean: -19.936337\n",
            "resetting env. episode 734.000000, reward total was -19.000000. running mean: -19.926974\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -19.937704\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -19.948327\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -19.948844\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -19.949355\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -19.959862\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -19.970263\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -19.980561\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -19.970755\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -19.981047\n",
            "resetting env. episode 744.000000, reward total was -19.000000. running mean: -19.971237\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -19.981525\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -19.991709\n",
            "resetting env. episode 747.000000, reward total was -18.000000. running mean: -19.971792\n",
            "resetting env. episode 748.000000, reward total was -20.000000. running mean: -19.972074\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -19.972354\n",
            "resetting env. episode 750.000000, reward total was -19.000000. running mean: -19.962630\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -19.973004\n",
            "resetting env. episode 752.000000, reward total was -18.000000. running mean: -19.953274\n",
            "resetting env. episode 753.000000, reward total was -19.000000. running mean: -19.943741\n",
            "resetting env. episode 754.000000, reward total was -19.000000. running mean: -19.934304\n",
            "resetting env. episode 755.000000, reward total was -16.000000. running mean: -19.894961\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -19.906011\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -19.906951\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -19.917881\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -19.928703\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -19.939415\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -19.940021\n",
            "resetting env. episode 762.000000, reward total was -19.000000. running mean: -19.930621\n",
            "resetting env. episode 763.000000, reward total was -18.000000. running mean: -19.911315\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -19.912202\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -19.913080\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -19.913949\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -19.924809\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -19.935561\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -19.926206\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -19.936944\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -19.947574\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -19.958099\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -19.958518\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -19.958932\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -19.969343\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -19.969650\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -19.979953\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -19.980154\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.970352\n",
            "resetting env. episode 780.000000, reward total was -19.000000. running mean: -19.960649\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -19.961042\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -19.971432\n",
            "resetting env. episode 783.000000, reward total was -19.000000. running mean: -19.961717\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -19.962100\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -19.962479\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -19.962854\n",
            "resetting env. episode 787.000000, reward total was -19.000000. running mean: -19.953226\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -19.963694\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -19.964057\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -19.964416\n",
            "resetting env. episode 791.000000, reward total was -17.000000. running mean: -19.934772\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -19.935424\n",
            "resetting env. episode 793.000000, reward total was -18.000000. running mean: -19.916070\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -19.916909\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -19.927740\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -19.938463\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -19.939078\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -19.949687\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -19.960190\n",
            "resetting env. episode 800.000000, reward total was -18.000000. running mean: -19.940589\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -19.951183\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -19.961671\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -19.962054\n",
            "resetting env. episode 804.000000, reward total was -19.000000. running mean: -19.952434\n",
            "resetting env. episode 805.000000, reward total was -16.000000. running mean: -19.912909\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -19.913780\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -19.904642\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -19.895596\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -19.896640\n",
            "resetting env. episode 810.000000, reward total was -18.000000. running mean: -19.877674\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -19.868897\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -19.880208\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -19.891406\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -19.882492\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -19.883667\n",
            "resetting env. episode 816.000000, reward total was -20.000000. running mean: -19.884830\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -19.885982\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -19.887122\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -19.898251\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -19.909268\n",
            "resetting env. episode 821.000000, reward total was -20.000000. running mean: -19.910176\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -19.911074\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -19.921963\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -19.912743\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -19.923616\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -19.934380\n",
            "resetting env. episode 827.000000, reward total was -19.000000. running mean: -19.925036\n",
            "resetting env. episode 828.000000, reward total was -19.000000. running mean: -19.915786\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -19.916628\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -19.927462\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -19.918187\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -19.919005\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -19.919815\n",
            "resetting env. episode 834.000000, reward total was -19.000000. running mean: -19.910617\n",
            "resetting env. episode 835.000000, reward total was -17.000000. running mean: -19.881511\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -19.882696\n",
            "resetting env. episode 837.000000, reward total was -18.000000. running mean: -19.863869\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -19.875230\n",
            "resetting env. episode 839.000000, reward total was -18.000000. running mean: -19.856478\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -19.867913\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -19.879234\n",
            "resetting env. episode 842.000000, reward total was -20.000000. running mean: -19.880441\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -19.881637\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -19.892821\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -19.893892\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -19.894954\n",
            "resetting env. episode 847.000000, reward total was -18.000000. running mean: -19.876004\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -19.877244\n",
            "resetting env. episode 849.000000, reward total was -19.000000. running mean: -19.868472\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -19.869787\n",
            "resetting env. episode 851.000000, reward total was -19.000000. running mean: -19.861089\n",
            "resetting env. episode 852.000000, reward total was -19.000000. running mean: -19.852478\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -19.853953\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -19.855414\n",
            "resetting env. episode 855.000000, reward total was -19.000000. running mean: -19.846860\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -19.848391\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -19.849907\n",
            "resetting env. episode 858.000000, reward total was -17.000000. running mean: -19.821408\n",
            "resetting env. episode 859.000000, reward total was -18.000000. running mean: -19.803194\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -19.805162\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -19.817110\n",
            "resetting env. episode 862.000000, reward total was -18.000000. running mean: -19.798939\n",
            "resetting env. episode 863.000000, reward total was -18.000000. running mean: -19.780950\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -19.773140\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -19.785409\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -19.797555\n",
            "resetting env. episode 867.000000, reward total was -18.000000. running mean: -19.779579\n",
            "resetting env. episode 868.000000, reward total was -19.000000. running mean: -19.771784\n",
            "resetting env. episode 869.000000, reward total was -19.000000. running mean: -19.764066\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -19.766425\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -19.768761\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -19.771073\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -19.773362\n",
            "resetting env. episode 874.000000, reward total was -18.000000. running mean: -19.755629\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -19.758073\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -19.770492\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -19.772787\n",
            "resetting env. episode 878.000000, reward total was -19.000000. running mean: -19.765059\n",
            "resetting env. episode 879.000000, reward total was -18.000000. running mean: -19.747408\n",
            "resetting env. episode 880.000000, reward total was -18.000000. running mean: -19.729934\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -19.742635\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -19.755209\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -19.757657\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -19.760080\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -19.762479\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -19.764854\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -19.777206\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -19.779434\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -19.781639\n",
            "resetting env. episode 890.000000, reward total was -18.000000. running mean: -19.763823\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -19.766185\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -19.768523\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -19.770838\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -19.773129\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -19.765398\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -19.767744\n",
            "resetting env. episode 897.000000, reward total was -19.000000. running mean: -19.760067\n",
            "resetting env. episode 898.000000, reward total was -19.000000. running mean: -19.752466\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -19.754941\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -19.757392\n",
            "resetting env. episode 901.000000, reward total was -19.000000. running mean: -19.749818\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -19.752320\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -19.764797\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -19.777149\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -19.779377\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -19.791583\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -19.783668\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -19.785831\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -19.787973\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -19.790093\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -19.782192\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -19.784370\n",
            "resetting env. episode 913.000000, reward total was -18.000000. running mean: -19.766526\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -19.768861\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -19.771172\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -19.783461\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -19.795626\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -19.807670\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -19.809593\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -19.811497\n",
            "resetting env. episode 921.000000, reward total was -20.000000. running mean: -19.813382\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -19.815248\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -19.827096\n",
            "resetting env. episode 924.000000, reward total was -19.000000. running mean: -19.818825\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -19.830637\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -19.842330\n",
            "resetting env. episode 927.000000, reward total was -18.000000. running mean: -19.823907\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -19.835668\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -19.837311\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -19.838938\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -19.850549\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -19.842043\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -19.853623\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -19.855087\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -19.856536\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -19.857970\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -19.859391\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -19.860797\n",
            "resetting env. episode 939.000000, reward total was -19.000000. running mean: -19.852189\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -19.853667\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -19.855130\n",
            "resetting env. episode 942.000000, reward total was -19.000000. running mean: -19.846579\n",
            "resetting env. episode 943.000000, reward total was -17.000000. running mean: -19.818113\n",
            "resetting env. episode 944.000000, reward total was -19.000000. running mean: -19.809932\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -19.811833\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -19.823714\n",
            "resetting env. episode 947.000000, reward total was -18.000000. running mean: -19.805477\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -19.817423\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -19.829248\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -19.840956\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -19.852546\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -19.844021\n",
            "resetting env. episode 953.000000, reward total was -19.000000. running mean: -19.835581\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -19.827225\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -19.828953\n",
            "resetting env. episode 956.000000, reward total was -18.000000. running mean: -19.810663\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -19.822556\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -19.834331\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -19.835987\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -19.847628\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -19.839151\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -19.850760\n",
            "resetting env. episode 963.000000, reward total was -18.000000. running mean: -19.832252\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -19.843930\n",
            "resetting env. episode 965.000000, reward total was -18.000000. running mean: -19.825490\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -19.827236\n",
            "resetting env. episode 967.000000, reward total was -18.000000. running mean: -19.808963\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -19.810874\n",
            "resetting env. episode 969.000000, reward total was -18.000000. running mean: -19.792765\n",
            "resetting env. episode 970.000000, reward total was -18.000000. running mean: -19.774837\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -19.787089\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -19.779218\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -19.781426\n",
            "resetting env. episode 974.000000, reward total was -18.000000. running mean: -19.763611\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -19.765975\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -19.778316\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -19.780532\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -19.772727\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -19.775000\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -19.787250\n",
            "resetting env. episode 981.000000, reward total was -19.000000. running mean: -19.779377\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -19.791584\n",
            "resetting env. episode 983.000000, reward total was -20.000000. running mean: -19.793668\n",
            "resetting env. episode 984.000000, reward total was -16.000000. running mean: -19.755731\n",
            "resetting env. episode 985.000000, reward total was -19.000000. running mean: -19.748174\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -19.760692\n",
            "resetting env. episode 987.000000, reward total was -18.000000. running mean: -19.743085\n",
            "resetting env. episode 988.000000, reward total was -18.000000. running mean: -19.725654\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -19.718398\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -19.721214\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -19.714002\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -19.716862\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -19.729693\n",
            "resetting env. episode 994.000000, reward total was -20.000000. running mean: -19.732396\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -19.745072\n",
            "resetting env. episode 996.000000, reward total was -19.000000. running mean: -19.737621\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -19.730245\n",
            "resetting env. episode 998.000000, reward total was -19.000000. running mean: -19.722943\n",
            "resetting env. episode 999.000000, reward total was -18.000000. running mean: -19.705713\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -19.718656\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -19.731470\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -19.744155\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -19.746713\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -19.749246\n",
            "resetting env. episode 1005.000000, reward total was -17.000000. running mean: -19.721754\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -19.734536\n",
            "resetting env. episode 1007.000000, reward total was -18.000000. running mean: -19.717191\n",
            "resetting env. episode 1008.000000, reward total was -19.000000. running mean: -19.710019\n",
            "resetting env. episode 1009.000000, reward total was -16.000000. running mean: -19.672919\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -19.676190\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -19.689428\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -19.682533\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -19.695708\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -19.708751\n",
            "resetting env. episode 1015.000000, reward total was -19.000000. running mean: -19.701663\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -19.694647\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -19.707700\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -19.710623\n",
            "resetting env. episode 1019.000000, reward total was -16.000000. running mean: -19.673517\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -19.676782\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -19.690014\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -19.703114\n",
            "resetting env. episode 1023.000000, reward total was -18.000000. running mean: -19.686083\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -19.699222\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -19.702230\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -19.705207\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -19.718155\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -19.730974\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -19.743664\n",
            "resetting env. episode 1030.000000, reward total was -18.000000. running mean: -19.726227\n",
            "resetting env. episode 1031.000000, reward total was -20.000000. running mean: -19.728965\n",
            "resetting env. episode 1032.000000, reward total was -18.000000. running mean: -19.711676\n",
            "resetting env. episode 1033.000000, reward total was -18.000000. running mean: -19.694559\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -19.707613\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -19.710537\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -19.713432\n",
            "resetting env. episode 1037.000000, reward total was -18.000000. running mean: -19.696297\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -19.689334\n",
            "resetting env. episode 1039.000000, reward total was -19.000000. running mean: -19.682441\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -19.695617\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -19.708660\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -19.711574\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -19.714458\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -19.717314\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -19.730140\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -19.742839\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -19.755411\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -19.767857\n",
            "resetting env. episode 1049.000000, reward total was -19.000000. running mean: -19.760178\n",
            "resetting env. episode 1050.000000, reward total was -18.000000. running mean: -19.742576\n",
            "resetting env. episode 1051.000000, reward total was -19.000000. running mean: -19.735150\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -19.747799\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -19.760321\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -19.752718\n",
            "resetting env. episode 1055.000000, reward total was -19.000000. running mean: -19.745191\n",
            "resetting env. episode 1056.000000, reward total was -19.000000. running mean: -19.737739\n",
            "resetting env. episode 1057.000000, reward total was -18.000000. running mean: -19.720361\n",
            "resetting env. episode 1058.000000, reward total was -17.000000. running mean: -19.693158\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -19.706226\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -19.709164\n",
            "resetting env. episode 1061.000000, reward total was -18.000000. running mean: -19.692072\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -19.695151\n",
            "resetting env. episode 1063.000000, reward total was -17.000000. running mean: -19.668200\n",
            "resetting env. episode 1064.000000, reward total was -18.000000. running mean: -19.651518\n",
            "resetting env. episode 1065.000000, reward total was -19.000000. running mean: -19.645003\n",
            "resetting env. episode 1066.000000, reward total was -19.000000. running mean: -19.638553\n",
            "resetting env. episode 1067.000000, reward total was -19.000000. running mean: -19.632167\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -19.635846\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -19.639487\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -19.643092\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -19.636661\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -19.630295\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -19.643992\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -19.657552\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -19.670976\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -19.684267\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -19.697424\n",
            "resetting env. episode 1078.000000, reward total was -19.000000. running mean: -19.690450\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -19.703545\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -19.706510\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -19.709445\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -19.722350\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.725127\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -19.737875\n",
            "resetting env. episode 1085.000000, reward total was -18.000000. running mean: -19.720497\n",
            "resetting env. episode 1086.000000, reward total was -19.000000. running mean: -19.713292\n",
            "resetting env. episode 1087.000000, reward total was -19.000000. running mean: -19.706159\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -19.719097\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -19.711906\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -19.704787\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -19.717739\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -19.720562\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -19.733356\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -19.746023\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -19.758562\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -19.770977\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -19.783267\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -19.795434\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -19.807480\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -19.799405\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -19.791411\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -19.773497\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -19.775762\n",
            "resetting env. episode 1104.000000, reward total was -18.000000. running mean: -19.758004\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -19.770424\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -19.782720\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -19.794893\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -19.806944\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -19.818875\n",
            "resetting env. episode 1110.000000, reward total was -18.000000. running mean: -19.800686\n",
            "resetting env. episode 1111.000000, reward total was -19.000000. running mean: -19.792679\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -19.794752\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -19.806805\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -19.818737\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.820549\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -19.832344\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -19.834020\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -19.835680\n",
            "resetting env. episode 1119.000000, reward total was -19.000000. running mean: -19.827323\n",
            "resetting env. episode 1120.000000, reward total was -18.000000. running mean: -19.809050\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -19.810960\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -19.812850\n",
            "resetting env. episode 1123.000000, reward total was -19.000000. running mean: -19.804722\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -19.816674\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -19.818508\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -19.830322\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -19.842019\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -19.833599\n",
            "resetting env. episode 1129.000000, reward total was -19.000000. running mean: -19.825263\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -19.817010\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -19.828840\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -19.840552\n",
            "resetting env. episode 1133.000000, reward total was -19.000000. running mean: -19.832146\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -19.833825\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -19.835487\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -19.837132\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -19.838761\n",
            "resetting env. episode 1138.000000, reward total was -19.000000. running mean: -19.830373\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -19.842069\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -19.853648\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -19.865112\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -19.876461\n",
            "resetting env. episode 1143.000000, reward total was -19.000000. running mean: -19.867696\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -19.869019\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -19.880329\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -19.891526\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -19.892611\n",
            "resetting env. episode 1148.000000, reward total was -18.000000. running mean: -19.873684\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -19.874948\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -19.876198\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -19.877436\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -19.878662\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -19.889875\n",
            "resetting env. episode 1154.000000, reward total was -18.000000. running mean: -19.870976\n",
            "resetting env. episode 1155.000000, reward total was -19.000000. running mean: -19.862267\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -19.863644\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -19.875008\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -19.876257\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -19.887495\n",
            "resetting env. episode 1160.000000, reward total was -18.000000. running mean: -19.868620\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -19.859934\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -19.861334\n",
            "resetting env. episode 1163.000000, reward total was -17.000000. running mean: -19.832721\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -19.824394\n",
            "resetting env. episode 1165.000000, reward total was -19.000000. running mean: -19.816150\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -19.827988\n",
            "resetting env. episode 1167.000000, reward total was -15.000000. running mean: -19.779709\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -19.781911\n",
            "resetting env. episode 1169.000000, reward total was -18.000000. running mean: -19.764092\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.776451\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -19.788687\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.790800\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.802892\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -19.814863\n",
            "resetting env. episode 1175.000000, reward total was -19.000000. running mean: -19.806714\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -19.818647\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -19.820461\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -19.822256\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -19.834034\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -19.845693\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -19.847236\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.848764\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -19.860276\n",
            "resetting env. episode 1184.000000, reward total was -17.000000. running mean: -19.831674\n",
            "resetting env. episode 1185.000000, reward total was -19.000000. running mean: -19.823357\n",
            "resetting env. episode 1186.000000, reward total was -16.000000. running mean: -19.785123\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -19.797272\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -19.809299\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -19.811206\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -19.813094\n",
            "resetting env. episode 1191.000000, reward total was -19.000000. running mean: -19.804963\n",
            "resetting env. episode 1192.000000, reward total was -19.000000. running mean: -19.796914\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -19.798945\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -19.800955\n",
            "resetting env. episode 1195.000000, reward total was -19.000000. running mean: -19.792946\n",
            "resetting env. episode 1196.000000, reward total was -19.000000. running mean: -19.785016\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -19.797166\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -19.799194\n",
            "resetting env. episode 1199.000000, reward total was -16.000000. running mean: -19.761202\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -19.773590\n",
            "resetting env. episode 1201.000000, reward total was -18.000000. running mean: -19.755854\n",
            "resetting env. episode 1202.000000, reward total was -19.000000. running mean: -19.748296\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -19.740813\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -19.753405\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -19.765871\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -19.768212\n",
            "resetting env. episode 1207.000000, reward total was -18.000000. running mean: -19.750530\n",
            "resetting env. episode 1208.000000, reward total was -18.000000. running mean: -19.733025\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -19.745694\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -19.748237\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -19.760755\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -19.753148\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -19.745616\n",
            "resetting env. episode 1214.000000, reward total was -17.000000. running mean: -19.718160\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -19.730978\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -19.743669\n",
            "resetting env. episode 1217.000000, reward total was -21.000000. running mean: -19.756232\n",
            "resetting env. episode 1218.000000, reward total was -18.000000. running mean: -19.738670\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -19.751283\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.763770\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -19.776132\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -19.788371\n",
            "resetting env. episode 1223.000000, reward total was -16.000000. running mean: -19.750487\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -19.752982\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -19.765453\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -19.757798\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -19.760220\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -19.772618\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -19.784892\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -19.797043\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -19.809072\n",
            "resetting env. episode 1232.000000, reward total was -18.000000. running mean: -19.790982\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.803072\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -19.815041\n",
            "resetting env. episode 1235.000000, reward total was -19.000000. running mean: -19.806891\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.808822\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -19.810734\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -19.812626\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -19.824500\n",
            "resetting env. episode 1240.000000, reward total was -17.000000. running mean: -19.796255\n",
            "resetting env. episode 1241.000000, reward total was -19.000000. running mean: -19.788292\n",
            "resetting env. episode 1242.000000, reward total was -18.000000. running mean: -19.770409\n",
            "resetting env. episode 1243.000000, reward total was -18.000000. running mean: -19.752705\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -19.765178\n",
            "resetting env. episode 1245.000000, reward total was -15.000000. running mean: -19.717527\n",
            "resetting env. episode 1246.000000, reward total was -18.000000. running mean: -19.700351\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -19.693348\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -19.706414\n",
            "resetting env. episode 1249.000000, reward total was -19.000000. running mean: -19.699350\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -19.692357\n",
            "resetting env. episode 1251.000000, reward total was -19.000000. running mean: -19.685433\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -19.698579\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -19.711593\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -19.724477\n",
            "resetting env. episode 1255.000000, reward total was -21.000000. running mean: -19.737232\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -19.739860\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -19.752461\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -19.744937\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -19.747487\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -19.750012\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -19.742512\n",
            "resetting env. episode 1262.000000, reward total was -18.000000. running mean: -19.725087\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -19.737836\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -19.750458\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -19.762953\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -19.765324\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -19.767671\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -19.769994\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -19.772294\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -19.784571\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -19.796725\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -19.788758\n",
            "resetting env. episode 1273.000000, reward total was -18.000000. running mean: -19.770871\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -19.783162\n",
            "resetting env. episode 1275.000000, reward total was -17.000000. running mean: -19.755330\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -19.757777\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -19.760199\n",
            "resetting env. episode 1278.000000, reward total was -16.000000. running mean: -19.722597\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.735371\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -19.738017\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -19.750637\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -19.743131\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -19.745700\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -19.758243\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -19.770660\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -19.782954\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -19.785124\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -19.797273\n",
            "resetting env. episode 1289.000000, reward total was -18.000000. running mean: -19.779300\n",
            "resetting env. episode 1290.000000, reward total was -19.000000. running mean: -19.771507\n",
            "resetting env. episode 1291.000000, reward total was -18.000000. running mean: -19.753792\n",
            "resetting env. episode 1292.000000, reward total was -19.000000. running mean: -19.746254\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -19.748792\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -19.741304\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -19.743891\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -19.756452\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.768887\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -19.781198\n",
            "resetting env. episode 1299.000000, reward total was -18.000000. running mean: -19.763386\n",
            "resetting env. episode 1300.000000, reward total was -16.000000. running mean: -19.725752\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -19.718495\n",
            "resetting env. episode 1302.000000, reward total was -19.000000. running mean: -19.711310\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -19.724197\n",
            "resetting env. episode 1304.000000, reward total was -17.000000. running mean: -19.696955\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.699985\n",
            "resetting env. episode 1306.000000, reward total was -20.000000. running mean: -19.702985\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -19.705956\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -19.718896\n",
            "resetting env. episode 1309.000000, reward total was -17.000000. running mean: -19.691707\n",
            "resetting env. episode 1310.000000, reward total was -17.000000. running mean: -19.664790\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -19.668142\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -19.681461\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -19.684646\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.697800\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -19.690822\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -19.693913\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -19.706974\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -19.719905\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -19.732706\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -19.735378\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -19.738025\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -19.740644\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -19.753238\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -19.765706\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -19.768049\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -19.780368\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -19.782564\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -19.784739\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -19.796891\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -19.808922\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -19.810833\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -19.802725\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -19.804698\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -19.816651\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -19.828484\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -19.830199\n",
            "resetting env. episode 1337.000000, reward total was -18.000000. running mean: -19.811897\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -19.813778\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -19.825641\n",
            "resetting env. episode 1340.000000, reward total was -19.000000. running mean: -19.817384\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -19.819210\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -19.821018\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -19.832808\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -19.834480\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -19.826135\n",
            "resetting env. episode 1346.000000, reward total was -18.000000. running mean: -19.807874\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -19.809795\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -19.801697\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -19.803680\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -19.805643\n",
            "resetting env. episode 1351.000000, reward total was -19.000000. running mean: -19.797587\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -19.789611\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -19.781715\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -19.783898\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -19.786059\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -19.798198\n",
            "resetting env. episode 1357.000000, reward total was -17.000000. running mean: -19.770216\n",
            "resetting env. episode 1358.000000, reward total was -19.000000. running mean: -19.762514\n",
            "resetting env. episode 1359.000000, reward total was -18.000000. running mean: -19.744889\n",
            "resetting env. episode 1360.000000, reward total was -20.000000. running mean: -19.747440\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -19.739966\n",
            "resetting env. episode 1362.000000, reward total was -19.000000. running mean: -19.732566\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -19.745240\n",
            "resetting env. episode 1364.000000, reward total was -18.000000. running mean: -19.727788\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -19.720510\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -19.733305\n",
            "resetting env. episode 1367.000000, reward total was -20.000000. running mean: -19.735972\n",
            "resetting env. episode 1368.000000, reward total was -17.000000. running mean: -19.708612\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -19.711526\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -19.724411\n",
            "resetting env. episode 1371.000000, reward total was -20.000000. running mean: -19.727167\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -19.739895\n",
            "resetting env. episode 1373.000000, reward total was -19.000000. running mean: -19.732496\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -19.745171\n",
            "resetting env. episode 1375.000000, reward total was -17.000000. running mean: -19.717719\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -19.730542\n",
            "resetting env. episode 1377.000000, reward total was -16.000000. running mean: -19.693237\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -19.706304\n",
            "resetting env. episode 1379.000000, reward total was -20.000000. running mean: -19.709241\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -19.712149\n",
            "resetting env. episode 1381.000000, reward total was -18.000000. running mean: -19.695027\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -19.698077\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -19.701096\n",
            "resetting env. episode 1384.000000, reward total was -19.000000. running mean: -19.694085\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -19.697145\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.710173\n",
            "resetting env. episode 1387.000000, reward total was -19.000000. running mean: -19.703071\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -19.716041\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -19.718880\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -19.721691\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -19.724475\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -19.727230\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -19.739958\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -19.742558\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -19.755132\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -19.767581\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -19.779905\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -19.792106\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -19.794185\n",
            "resetting env. episode 1400.000000, reward total was -17.000000. running mean: -19.766243\n",
            "resetting env. episode 1401.000000, reward total was -17.000000. running mean: -19.738581\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -19.741195\n",
            "resetting env. episode 1403.000000, reward total was -19.000000. running mean: -19.733783\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -19.746445\n",
            "resetting env. episode 1405.000000, reward total was -17.000000. running mean: -19.718981\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -19.721791\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -19.724573\n",
            "resetting env. episode 1408.000000, reward total was -19.000000. running mean: -19.717327\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -19.730154\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -19.732853\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -19.745524\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -19.748069\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -19.750588\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -19.753082\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -19.745551\n",
            "resetting env. episode 1416.000000, reward total was -19.000000. running mean: -19.738096\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -19.740715\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -19.743308\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -19.735875\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -19.748516\n",
            "resetting env. episode 1421.000000, reward total was -19.000000. running mean: -19.741031\n",
            "resetting env. episode 1422.000000, reward total was -18.000000. running mean: -19.723620\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -19.726384\n",
            "resetting env. episode 1424.000000, reward total was -17.000000. running mean: -19.699120\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -19.692129\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -19.695208\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.708256\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -19.711173\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -19.714062\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -19.726921\n",
            "resetting env. episode 1431.000000, reward total was -18.000000. running mean: -19.709652\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -19.722555\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -19.725330\n",
            "resetting env. episode 1434.000000, reward total was -18.000000. running mean: -19.708076\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.720996\n",
            "resetting env. episode 1436.000000, reward total was -19.000000. running mean: -19.713786\n",
            "resetting env. episode 1437.000000, reward total was -18.000000. running mean: -19.696648\n",
            "resetting env. episode 1438.000000, reward total was -19.000000. running mean: -19.689681\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -19.702784\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -19.705757\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -19.718699\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -19.731512\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -19.734197\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -19.746855\n",
            "resetting env. episode 1445.000000, reward total was -19.000000. running mean: -19.739386\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -19.741993\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -19.744573\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -19.737127\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -19.749756\n",
            "resetting env. episode 1450.000000, reward total was -18.000000. running mean: -19.732258\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -19.724936\n",
            "resetting env. episode 1452.000000, reward total was -19.000000. running mean: -19.717686\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -19.730509\n",
            "resetting env. episode 1454.000000, reward total was -19.000000. running mean: -19.723204\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.735972\n",
            "resetting env. episode 1456.000000, reward total was -18.000000. running mean: -19.718612\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -19.731426\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -19.734112\n",
            "resetting env. episode 1459.000000, reward total was -18.000000. running mean: -19.716771\n",
            "resetting env. episode 1460.000000, reward total was -17.000000. running mean: -19.689603\n",
            "resetting env. episode 1461.000000, reward total was -18.000000. running mean: -19.672707\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -19.685980\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -19.699120\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -19.712129\n",
            "resetting env. episode 1465.000000, reward total was -18.000000. running mean: -19.695008\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -19.708058\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -19.720977\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -19.713767\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -19.706630\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -19.709563\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.712468\n",
            "resetting env. episode 1472.000000, reward total was -18.000000. running mean: -19.695343\n",
            "resetting env. episode 1473.000000, reward total was -18.000000. running mean: -19.678390\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -19.681606\n",
            "resetting env. episode 1475.000000, reward total was -19.000000. running mean: -19.674790\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.688042\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -19.681161\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -19.694350\n",
            "resetting env. episode 1479.000000, reward total was -20.000000. running mean: -19.697406\n",
            "resetting env. episode 1480.000000, reward total was -18.000000. running mean: -19.680432\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -19.693628\n",
            "resetting env. episode 1482.000000, reward total was -15.000000. running mean: -19.646692\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -19.650225\n",
            "resetting env. episode 1484.000000, reward total was -20.000000. running mean: -19.653722\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -19.667185\n",
            "resetting env. episode 1486.000000, reward total was -19.000000. running mean: -19.660513\n",
            "resetting env. episode 1487.000000, reward total was -18.000000. running mean: -19.643908\n",
            "resetting env. episode 1488.000000, reward total was -17.000000. running mean: -19.617469\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -19.631294\n",
            "resetting env. episode 1490.000000, reward total was -19.000000. running mean: -19.624982\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.638732\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -19.652344\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -19.665821\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -19.679163\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.682371\n",
            "resetting env. episode 1496.000000, reward total was -16.000000. running mean: -19.645547\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -19.639092\n",
            "resetting env. episode 1498.000000, reward total was -18.000000. running mean: -19.622701\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.626474\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -19.620209\n",
            "CPU times: user 6h 43min 48s, sys: 59min 43s, total: 7h 43min 32s\n",
            "Wall time: 3h 58min 35s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2NblmwDsL3y",
        "outputId": "77eb2b58-16cf-4d9d-818f-8f0578aa1f42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "neurons 800.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}