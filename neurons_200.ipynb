{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neurons 200.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "3ce7453e-d4dc-423f-d3ac-e9f56a1deb64"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 28.6 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=fb0fb7d2fcc8711940e3f8deec547f38c7b39f75a601e33434e661485550d77e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "6ef63e23-622a-4907-d1de-b081e627fb66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "a0857e17-64b7-4746-e4c3-a5fad6c8074a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "970d405e-e155-4f24-ba0d-0986907f4bc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "e8293bad-2437-4599-ff4f-1562b97a4fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "21c59a5a-d172-426d-a6d9-4fad3d1941fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -18.000000. running mean: -20.970000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.970300\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.970597\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.970891\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.961182\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.961570\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.961955\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.952335\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.952812\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.943284\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.943851\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.944412\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.924968\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -20.905718\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.906661\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.897595\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.898619\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.899633\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.900636\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.901630\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.902614\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.903587\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.894552\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.895606\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.896650\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.887683\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.888807\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.889919\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.881019\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.872209\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.873487\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.874752\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.876005\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.877245\n",
            "resetting env. episode 37.000000, reward total was -18.000000. running mean: -20.848472\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.849987\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.851488\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.842973\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.834543\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.836198\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.827836\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.829557\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.831262\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.832949\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.834620\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.836273\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.827911\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.819632\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.811435\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.813321\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.815188\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.817036\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.818865\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.810677\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.812570\n",
            "resetting env. episode 58.000000, reward total was -18.000000. running mean: -20.784444\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.786600\n",
            "resetting env. episode 60.000000, reward total was -19.000000. running mean: -20.768734\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.751046\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.753536\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.756001\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.758441\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.760856\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.753248\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.755715\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.748158\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.750676\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.753170\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.755638\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.748082\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.730601\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.713295\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.716162\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.719000\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.721810\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.724592\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.727346\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.720073\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.712872\n",
            "resetting env. episode 82.000000, reward total was -19.000000. running mean: -20.695743\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.698786\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.691798\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.684880\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.688031\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.691151\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.694239\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.687297\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.690424\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.693520\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -20.676585\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.679819\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.683021\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.676190\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.659428\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.662834\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.656206\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.639644\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.643247\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.646815\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.650347\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -20.623843\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.627605\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.621329\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.605115\n",
            "resetting env. episode 107.000000, reward total was -17.000000. running mean: -20.569064\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.573374\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.577640\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -20.551864\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.556345\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.560781\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.565174\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.559522\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.553927\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.558387\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.562804\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.567176\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.571504\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.565789\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -20.550131\n",
            "resetting env. episode 122.000000, reward total was -18.000000. running mean: -20.524630\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.529383\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.534089\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.528749\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.533461\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.528126\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.512845\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.517717\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.512540\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.517414\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.522240\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.517018\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.511847\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.516729\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.521562\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.526346\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.531083\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.535772\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.520414\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.525210\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.529958\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.534658\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.539312\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.523919\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.528679\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.513393\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.518259\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.523076\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.507845\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.512767\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.517639\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.512463\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.517338\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.512165\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.517043\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.521873\n",
            "resetting env. episode 158.000000, reward total was -18.000000. running mean: -20.496654\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.501687\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.506671\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.481604\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.486788\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.491920\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.487001\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.482131\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.477309\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.472536\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.477811\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.463033\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.458403\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.453818\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.459280\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.454687\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.450141\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.455639\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.451083\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.456572\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.462006\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.467386\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.472712\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.477985\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.463205\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.458573\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.463988\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.459348\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.454754\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.460207\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.455605\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.461049\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.466438\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.471774\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.477056\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.482285\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.487463\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.492588\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.497662\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.502685\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.507659\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.512582\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.507456\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.512382\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.507258\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.492185\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.497263\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.502291\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.507268\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.502195\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.487173\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.492301\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.487378\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.482505\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.467680\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.473003\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.478273\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.483490\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.468655\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.463969\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.459329\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.464736\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.460088\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.465487\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.470833\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.466124\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.471463\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.476748\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.461981\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.457361\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.462787\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.468160\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.473478\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.458743\n",
            "resetting env. episode 232.000000, reward total was -19.000000. running mean: -20.444156\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.449714\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.445217\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.450765\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.456257\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.461695\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.467078\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.472407\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.477683\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.482906\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.488077\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.493196\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.498264\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.493282\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.498349\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.493365\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.488432\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.483547\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.488712\n",
            "resetting env. episode 251.000000, reward total was -18.000000. running mean: -20.463825\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.469186\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.474495\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.479750\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.484952\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.480103\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.485302\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.490449\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.485544\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.480689\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.485882\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.491023\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.496113\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.501152\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.506140\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.511079\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.515968\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.520808\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.525600\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.530344\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.535041\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.539690\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.514293\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.509150\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.514059\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.518918\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.513729\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.498592\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.493606\n",
            "resetting env. episode 280.000000, reward total was -18.000000. running mean: -20.468670\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.453983\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.459443\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.464849\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.440200\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.425798\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.431540\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.427225\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.432953\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.438623\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.434237\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.439895\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.435496\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.441141\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.436729\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.432362\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.408038\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.413958\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.419818\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.415620\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.421464\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.427249\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.432977\n",
            "resetting env. episode 303.000000, reward total was -17.000000. running mean: -20.398647\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.404661\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.410614\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.406508\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.412443\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.418318\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.414135\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.399994\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.395994\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.392034\n",
            "resetting env. episode 313.000000, reward total was -18.000000. running mean: -20.368114\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.374433\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.380688\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.386881\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.383013\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.389182\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.395291\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.401338\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.387324\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.393451\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.399517\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.405521\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.391466\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.397552\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.393576\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.399640\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.405644\n",
            "resetting env. episode 330.000000, reward total was -18.000000. running mean: -20.381587\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.387772\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.373894\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.380155\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.386353\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.392490\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.388565\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.384679\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.370832\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.367124\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.373453\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.379718\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.385921\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.392062\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.398141\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.404160\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.400118\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.396117\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.402156\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.408134\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.414053\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.409913\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.415813\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.411655\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.417539\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.413363\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.399230\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.405237\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.401185\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.397173\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.403201\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.389169\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.375278\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.371525\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.357810\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.364232\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.370589\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.376883\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.373115\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.379383\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.385590\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.391734\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.397816\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.383838\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.390000\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.386100\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.392239\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.398316\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.394333\n",
            "resetting env. episode 379.000000, reward total was -18.000000. running mean: -20.370390\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.366686\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.373019\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.379289\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.385496\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.391641\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.387725\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.393847\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.399909\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.405910\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.411851\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.407732\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.403655\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.409618\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.395522\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.401567\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.397551\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.403576\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.409540\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.415445\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.401290\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.397277\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.393305\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.399372\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.405378\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.411324\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.407211\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.413139\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.419007\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.414817\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.420669\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.426462\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.422198\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.427976\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.423696\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.429459\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.425164\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.420913\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.426704\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.422437\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.428212\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.433930\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.439591\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.435195\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.440843\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.446435\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.441970\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.447551\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.453075\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.438544\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.434159\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.429817\n",
            "resetting env. episode 431.000000, reward total was -18.000000. running mean: -20.405519\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.411464\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.417349\n",
            "resetting env. episode 434.000000, reward total was -18.000000. running mean: -20.393176\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.399244\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.405252\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.411199\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.407087\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.403016\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.408986\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.404896\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.410847\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.406739\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.412671\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.408545\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.404459\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.400415\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.406410\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.402346\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.408323\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.404240\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.410197\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.416095\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.421934\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.427715\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.423438\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.429203\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.414911\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.410762\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.406655\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.412588\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.418462\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.424278\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.430035\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.435734\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.441377\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.446963\n",
            "resetting env. episode 468.000000, reward total was -18.000000. running mean: -20.422494\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.418269\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.424086\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.429845\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.425547\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.431291\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.426978\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.432709\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.438382\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.423998\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.419758\n",
            "resetting env. episode 479.000000, reward total was -19.000000. running mean: -20.405560\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.411505\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.407390\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.403316\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.409282\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.415190\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.421038\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.416827\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.402659\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.408633\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.414546\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.420401\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.406197\n",
            "resetting env. episode 492.000000, reward total was -18.000000. running mean: -20.382135\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.368313\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.374630\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.370884\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.377175\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.373403\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.359669\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.366073\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.372412\n",
            "CPU times: user 22min 55s, sys: 10min 36s, total: 33min 32s\n",
            "Wall time: 17min 22s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "11919987-5f78-4425-9897-cb30ed4025a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -18.000000. running mean: -20.960297\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.950694\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.951187\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951675\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.932158\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.932837\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.933509\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -20.914173\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.895032\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.896081\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.887121\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.878249\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.869467\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.860772\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.852164\n",
            "resetting env. episode 20.000000, reward total was -18.000000. running mean: -20.823643\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.805406\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.797352\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.789379\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.791485\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.783570\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.785734\n",
            "resetting env. episode 27.000000, reward total was -19.000000. running mean: -20.767877\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.760198\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.752596\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.745070\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.737620\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.740244\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.732841\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -20.715513\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.708358\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.711274\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.704161\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.707120\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.710048\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.702948\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.695918\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.698959\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.701970\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -20.674950\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.678200\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.671418\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.674704\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.677957\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -20.651178\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.654666\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.658119\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -20.631538\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.615223\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.609070\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.592980\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.587050\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -20.571179\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.555468\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.559913\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.564314\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.568671\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.562984\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.567354\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.551681\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.556164\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.560602\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.564996\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.569346\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.573653\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.577916\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.582137\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.586316\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.590453\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.584548\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.578703\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.582915\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.587086\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.591215\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.575303\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.569550\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.573855\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.578116\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.582335\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.566512\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.560847\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.555238\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.539686\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.544289\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.548846\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.553358\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.537824\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.542446\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.537021\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.531651\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.526335\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.531071\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.525760\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.520503\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.505298\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.500245\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.505242\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.500190\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.495188\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.500236\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.505234\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.510182\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.515080\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.509929\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.504830\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.509781\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.494684\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.489737\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.494839\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.499891\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.504892\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.509843\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.514745\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.499597\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.504601\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.509555\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.504460\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.509415\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.514321\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.519178\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.523986\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.518746\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.523559\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.518323\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.523140\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.527908\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.532629\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.527303\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.512030\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.516910\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.511741\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.516623\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.521457\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.526242\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.520980\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.525770\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.530512\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.535207\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.519855\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.524657\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.519410\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.524216\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.508974\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.503884\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.508845\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.513757\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.518619\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.523433\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.518199\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.513017\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.517887\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.512708\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.517581\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.522405\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.517181\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.522009\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -20.496789\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.501821\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.506803\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.511735\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.516617\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.521451\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.526237\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.520974\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.525765\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.510507\n",
            "resetting env. episode 171.000000, reward total was -18.000000. running mean: -20.485402\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.480548\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.485742\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.490885\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.495976\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.491016\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.486106\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.491245\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.496333\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.491369\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.486456\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.491591\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.476675\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.481908\n",
            "resetting env. episode 185.000000, reward total was -19.000000. running mean: -20.467089\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.472418\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.467694\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.473017\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.458287\n",
            "resetting env. episode 190.000000, reward total was -17.000000. running mean: -20.423704\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.409467\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.415373\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.401219\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.407207\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -20.393135\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.399203\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.405211\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.411159\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.417048\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.422877\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.428648\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.434362\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.440018\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.445618\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.451162\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.446650\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.442184\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.447762\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.453284\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.438751\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.444364\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.439920\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.425521\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.421266\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.417053\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.422883\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.428654\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.414367\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.410224\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.406121\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.402060\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.408040\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.413959\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.419820\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.425621\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.431365\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.427052\n",
            "resetting env. episode 228.000000, reward total was -18.000000. running mean: -20.402781\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.398753\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.404766\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.400718\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.406711\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.412644\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.408517\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.404432\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.410388\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.396284\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.392321\n",
            "resetting env. episode 239.000000, reward total was -18.000000. running mean: -20.368398\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.374714\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.370967\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.377257\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.383484\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.389650\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.395753\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.401796\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.397778\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.383800\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.389962\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.386062\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.392202\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.398280\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.404297\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.410254\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.406151\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.412090\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.397969\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.403989\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.409949\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.405850\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.401791\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.407773\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.403696\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.409659\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.405562\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.401507\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.407491\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.413417\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.409282\n",
            "resetting env. episode 270.000000, reward total was -17.000000. running mean: -20.375190\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.381438\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.387623\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.383747\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.389910\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.396010\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.392050\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.388130\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.384249\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.380406\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.386602\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.382736\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.388909\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.395020\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.401069\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.407059\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.392988\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.399058\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.385068\n",
            "resetting env. episode 289.000000, reward total was -18.000000. running mean: -20.361217\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.367605\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.363929\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.370289\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.376587\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.382821\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.388992\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.395103\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.381152\n",
            "resetting env. episode 298.000000, reward total was -16.000000. running mean: -20.337340\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.333967\n",
            "resetting env. episode 300.000000, reward total was -18.000000. running mean: -20.310627\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.307521\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.314445\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.311301\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.308188\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.315106\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.311955\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.318836\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.325647\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.312391\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.319267\n",
            "resetting env. episode 311.000000, reward total was -18.000000. running mean: -20.296074\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.303113\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.310082\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.316981\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.303812\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.290773\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.297866\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.294887\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.301938\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.288919\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.296030\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.303069\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.310039\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.316938\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.313769\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.310631\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.297525\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.304550\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.301504\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.308489\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.305404\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.312350\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.319227\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.316034\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.302874\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.309845\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.306747\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.313679\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.320543\n",
            "resetting env. episode 340.000000, reward total was -17.000000. running mean: -20.287337\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.274464\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.281719\n",
            "resetting env. episode 343.000000, reward total was -18.000000. running mean: -20.258902\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.266313\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.263650\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.261013\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.258403\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.255819\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.243261\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.240828\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.248420\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.245936\n",
            "resetting env. episode 353.000000, reward total was -18.000000. running mean: -20.223477\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.221242\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.229029\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.226739\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.224472\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.222227\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.230005\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.237705\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.245328\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.242874\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.250446\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.247941\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.255462\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.262907\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.260278\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.267675\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.274998\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.272248\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.279526\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.286731\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.293863\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.300925\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.297916\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.294936\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.291987\n",
            "resetting env. episode 378.000000, reward total was -19.000000. running mean: -20.279067\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.266276\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.273614\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.280878\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.268069\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.275388\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.262634\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.270008\n",
            "resetting env. episode 386.000000, reward total was -18.000000. running mean: -20.247308\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.234835\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.242486\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.240062\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.237661\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.245284\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.252831\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.240303\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.237900\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.225521\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.223266\n",
            "resetting env. episode 397.000000, reward total was -18.000000. running mean: -20.201033\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.209023\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.216933\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.224763\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.212516\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.210391\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.208287\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.206204\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.204142\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.202100\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.210079\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.217979\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.225799\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.223541\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.221305\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.229092\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.236801\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.244433\n",
            "resetting env. episode 415.000000, reward total was -17.000000. running mean: -20.211989\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.209869\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.207770\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.205693\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.203636\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.201599\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.209583\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.217488\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.205313\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.213260\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.221127\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.218916\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.226727\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.224459\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.222215\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.219993\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.207793\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.215715\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.203558\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.201522\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.199507\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.207512\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.215437\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.223282\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.221049\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.218839\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.216651\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.224484\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.222239\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.230017\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.227717\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.225439\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.233185\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.230853\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.238545\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.246159\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.233698\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.241361\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.238947\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.226558\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.234292\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.231949\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.219630\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.217433\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.225259\n",
            "resetting env. episode 460.000000, reward total was -18.000000. running mean: -20.203006\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.200976\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.208967\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.216877\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.214708\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.202561\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.200535\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.208530\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.216445\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.214280\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.222138\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.229916\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.227617\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.215341\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.213187\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.221056\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.228845\n",
            "resetting env. episode 477.000000, reward total was -18.000000. running mean: -20.206557\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.214491\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.212346\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.220223\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.218020\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.225840\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.233582\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.221246\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.219033\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.226843\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.224575\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.232329\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.240006\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.247606\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.235130\n",
            "resetting env. episode 492.000000, reward total was -18.000000. running mean: -20.212778\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.220651\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.228444\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.226160\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.213898\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.211759\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.209641\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.217545\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.225370\n",
            "CPU times: user 23min 32s, sys: 10min 50s, total: 34min 22s\n",
            "Wall time: 17min 46s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "ce3f9a22-73f3-40bb-ef1c-b8720e1c8fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "e7a3ee34-9a79-4f37-f2f0-10d4ca1a0ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049601\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -19.069105\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.088414\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -19.107530\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.126455\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -19.135190\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -19.143838\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -19.162400\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -19.160776\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -19.159168\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.167576\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.185900\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -19.204041\n",
            "resetting env. episode 17.000000, reward total was -18.000000. running mean: -19.192001\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -19.200081\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -19.198080\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -19.196099\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.204138\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.212097\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -19.209976\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -19.207876\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.225798\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.243540\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.261104\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -19.268493\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -19.275808\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.293050\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -19.310120\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -19.317018\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -19.313848\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.320710\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.327503\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -19.334228\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -19.340885\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.357477\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -19.353902\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.370363\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.376659\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.382893\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.399064\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.405073\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.421022\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.436812\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -19.452444\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -19.457919\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -19.473340\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.488607\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.493721\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -19.498784\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -19.513796\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -19.518658\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.533471\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.548136\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.562655\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -19.577029\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -19.591258\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.605346\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.609292\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.613199\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -19.617067\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -19.620897\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.634688\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.648341\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.661857\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.665239\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.668586\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.681901\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -19.675082\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.688331\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.701447\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.714433\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.727289\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.740016\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.752616\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.765089\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.767439\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.769764\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.772067\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.774346\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.786602\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.798736\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.810749\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.812642\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.814515\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.826370\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -19.818106\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.829925\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.841626\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.843210\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -19.834778\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.836430\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.848066\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.849585\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -19.841089\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.842678\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.854251\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.865709\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.867052\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.868381\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -19.859697\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.871100\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.882389\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.893566\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.894630\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.895684\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.906727\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.917659\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.918483\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.929298\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -19.940005\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.950605\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -19.951099\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.961588\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.971972\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.982252\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.992430\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.002506\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.012481\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.012356\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.012232\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.022110\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.021889\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.011670\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.021553\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.031338\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.041024\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.040614\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.050208\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.039706\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.039309\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.038916\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.038526\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.038141\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.047760\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.047282\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.056809\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.066241\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.055579\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.045023\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.044573\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -20.024127\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.023886\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.033647\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.033311\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.032977\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.022648\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.032421\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.022097\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.021876\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.031657\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.031341\n",
            "resetting env. episode 155.000000, reward total was -18.000000. running mean: -20.011027\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.010917\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.020808\n",
            "resetting env. episode 158.000000, reward total was -18.000000. running mean: -20.000600\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.000594\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.000588\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.010582\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.010476\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.010371\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.020268\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.020065\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.029864\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.029566\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.039270\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.038877\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.048489\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.058004\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.067424\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.076749\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.075982\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.075222\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.064470\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.063825\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.073187\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.082455\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.091630\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.100714\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.099707\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.088710\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.087823\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.096945\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.105975\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.104915\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.113866\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.122728\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.121500\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.120285\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.129082\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.137792\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.126414\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.125150\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.133898\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.142559\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.151134\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.159622\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.168026\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.176346\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.174582\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.182836\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.191008\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.199098\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.197107\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.205136\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.203085\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.201054\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.209043\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.216953\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.224783\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.222535\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.210310\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.218207\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.216025\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.223865\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.211626\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.219510\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.217315\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.225141\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.222890\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.220661\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.208455\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.206370\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.204306\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.212263\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.220141\n",
            "resetting env. episode 229.000000, reward total was -18.000000. running mean: -20.197939\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.205960\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.203900\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.211861\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.209743\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.217645\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.205469\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.213414\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.221280\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.219067\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.226876\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.224608\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.222362\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.220138\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.207937\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.205857\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.213799\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.211661\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.219544\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.227349\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.225075\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.232824\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.230496\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.238191\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.245809\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.233351\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.241018\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.248607\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.256121\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.253560\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.261025\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.258414\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.265830\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.253172\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.250640\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.238134\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.245752\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.253295\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.260762\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.268154\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.255473\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.252918\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.260389\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.267785\n",
            "resetting env. episode 273.000000, reward total was -18.000000. running mean: -20.245107\n",
            "resetting env. episode 274.000000, reward total was -16.000000. running mean: -20.202656\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.210630\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.218523\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.226338\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.234075\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.241734\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.249317\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.256823\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.264255\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.261613\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.258996\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.256406\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -20.233842\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.241504\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.239089\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.236698\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.234331\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.231988\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.239668\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.237271\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.244899\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.252450\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.249925\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.257426\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.264852\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.262203\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.259581\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.256985\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.264415\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.271771\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.259053\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.246463\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.233998\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.221658\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.219442\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.227247\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.234975\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.222625\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.220399\n",
            "resetting env. episode 313.000000, reward total was -18.000000. running mean: -20.198195\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.206213\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.214151\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.222009\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.229789\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.237491\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.245116\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.252665\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.260139\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.267537\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.274862\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.272113\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.279392\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.276598\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.273832\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.261094\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.268483\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.275798\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.283040\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.290210\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.297308\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.304334\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.311291\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.318178\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.324996\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.321746\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.328529\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.315244\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.322091\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.328870\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.335582\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.332226\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.338904\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.335515\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.342159\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.338738\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.345350\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.341897\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.328478\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.325193\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.321941\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.328722\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.335435\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.332080\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.338759\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.345372\n",
            "resetting env. episode 359.000000, reward total was -18.000000. running mean: -20.321918\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.318699\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.315512\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.312357\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.309233\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.316141\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.322980\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.329750\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.326452\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.313188\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.320056\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.326855\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.323587\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.330351\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.317047\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.313877\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.310738\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.317631\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.314454\n",
            "resetting env. episode 378.000000, reward total was -18.000000. running mean: -20.291310\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.298397\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.295413\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.302459\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.309434\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.316340\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.323176\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.309945\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.316845\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.323677\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.330440\n",
            "resetting env. episode 389.000000, reward total was -18.000000. running mean: -20.307136\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.314064\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.300924\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.307914\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.304835\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.311787\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.308669\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.305582\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.312526\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.319401\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.316207\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.313045\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.319915\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.326716\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.333448\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.340114\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.346713\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.343246\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.329813\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.336515\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.343150\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.349718\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.356221\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.342659\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.349232\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.355740\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.352183\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.358661\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.355074\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.361523\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.367908\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.374229\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.380487\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.386682\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.392815\n",
            "resetting env. episode 424.000000, reward total was -17.000000. running mean: -20.358887\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.365298\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.371645\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.357929\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.354349\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.360806\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.357198\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.343626\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.340190\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.336788\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.333420\n",
            "resetting env. episode 435.000000, reward total was -17.000000. running mean: -20.300086\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.307085\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.304014\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.300974\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.297964\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.304984\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.301935\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.308915\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.315826\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.312668\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.319541\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.326346\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.313082\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.319951\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.326752\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.333484\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.340150\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.336748\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.343381\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.339947\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.326547\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.333282\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.339949\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.346550\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.343084\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.339653\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.346257\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.352794\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.339266\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.335874\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.332515\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.339190\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.335798\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.342440\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.349015\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.345525\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.352070\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.358549\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.364964\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.361314\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.367701\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.374024\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.380284\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.386481\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.392616\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.388690\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.394803\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.380855\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.377046\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.383276\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.389443\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.375549\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.371793\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.378075\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.374295\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.380552\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.376746\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.362979\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.359349\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.365755\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.362098\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.348477\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.354992\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.361442\n",
            "resetting env. episode 499.000000, reward total was -18.000000. running mean: -20.337828\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.334449\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.331105\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.337794\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.344416\n",
            "resetting env. episode 504.000000, reward total was -19.000000. running mean: -20.330972\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.327662\n",
            "resetting env. episode 506.000000, reward total was -19.000000. running mean: -20.314386\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.321242\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.328029\n",
            "resetting env. episode 509.000000, reward total was -19.000000. running mean: -20.314749\n",
            "resetting env. episode 510.000000, reward total was -18.000000. running mean: -20.291601\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.288685\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.285799\n",
            "resetting env. episode 513.000000, reward total was -19.000000. running mean: -20.272941\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.270211\n",
            "resetting env. episode 515.000000, reward total was -19.000000. running mean: -20.257509\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.264934\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.262285\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.259662\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.267065\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.274395\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.281651\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.288834\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.295946\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.302986\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.299956\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.306957\n",
            "resetting env. episode 527.000000, reward total was -20.000000. running mean: -20.303887\n",
            "resetting env. episode 528.000000, reward total was -17.000000. running mean: -20.270848\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.278140\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.285359\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.292505\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.299580\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.306584\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.313518\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.310383\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -20.297279\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.304306\n",
            "resetting env. episode 538.000000, reward total was -19.000000. running mean: -20.291263\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.288351\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.295467\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.302513\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.299487\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.306493\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.303428\n",
            "resetting env. episode 545.000000, reward total was -19.000000. running mean: -20.290393\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.297489\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.304515\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -20.291469\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.298555\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.305569\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.302513\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.309488\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.296393\n",
            "resetting env. episode 554.000000, reward total was -19.000000. running mean: -20.283430\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.280595\n",
            "resetting env. episode 556.000000, reward total was -18.000000. running mean: -20.257789\n",
            "resetting env. episode 557.000000, reward total was -19.000000. running mean: -20.245211\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.242759\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -20.230332\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.228028\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.225748\n",
            "resetting env. episode 562.000000, reward total was -19.000000. running mean: -20.213491\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.221356\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.229142\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.226851\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.224582\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.232336\n",
            "resetting env. episode 568.000000, reward total was -19.000000. running mean: -20.220013\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.217813\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.225635\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.213378\n",
            "resetting env. episode 572.000000, reward total was -19.000000. running mean: -20.201245\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.199232\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.197240\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -20.195267\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.203315\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.211282\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.219169\n",
            "resetting env. episode 579.000000, reward total was -17.000000. running mean: -20.186977\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.185107\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.183256\n",
            "resetting env. episode 582.000000, reward total was -20.000000. running mean: -20.181424\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.179609\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -20.167813\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -20.166135\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.174474\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.182729\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.190902\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.188993\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.197103\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.205132\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.213081\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.220950\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.218740\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.226553\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.234287\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.231944\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.229625\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.237329\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.234955\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.232606\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.230280\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.227977\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.235697\n",
            "resetting env. episode 605.000000, reward total was -19.000000. running mean: -20.223340\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.231107\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.228796\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.226508\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.224243\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.232000\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.239680\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.247284\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.254811\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.252263\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.259740\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.267143\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.274471\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.281726\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.278909\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.276120\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.283359\n",
            "resetting env. episode 622.000000, reward total was -19.000000. running mean: -20.270525\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.277820\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.285042\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.282191\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -20.269370\n",
            "resetting env. episode 627.000000, reward total was -18.000000. running mean: -20.246676\n",
            "resetting env. episode 628.000000, reward total was -18.000000. running mean: -20.224209\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.211967\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.219847\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.227649\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.235372\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.243019\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.250588\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.258083\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.265502\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.262847\n",
            "resetting env. episode 638.000000, reward total was -18.000000. running mean: -20.240218\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.247816\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.245338\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.252885\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.260356\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.257752\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.255175\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.252623\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.260097\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.247496\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.255021\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.252471\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.259946\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.257346\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.264773\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.272125\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.269404\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.266710\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.264043\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.271402\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.278688\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.275901\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.283142\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.280311\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.287508\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.284633\n",
            "resetting env. episode 664.000000, reward total was -19.000000. running mean: -20.271786\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.279069\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.286278\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.293415\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.290481\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.297576\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.304600\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.311554\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.308439\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.315354\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.322201\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.328979\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.325689\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.332432\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.339108\n",
            "resetting env. episode 679.000000, reward total was -18.000000. running mean: -20.315717\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.312560\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.319434\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.326240\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.332977\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.339648\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.346251\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.352789\n",
            "resetting env. episode 687.000000, reward total was -19.000000. running mean: -20.339261\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.345868\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.352409\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.348885\n",
            "resetting env. episode 691.000000, reward total was -17.000000. running mean: -20.315396\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.312242\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.319120\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.315929\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.322770\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.329542\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -20.316246\n",
            "resetting env. episode 698.000000, reward total was -20.000000. running mean: -20.313084\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.319953\n",
            "resetting env. episode 700.000000, reward total was -18.000000. running mean: -20.296754\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.303786\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.310748\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.307641\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.314564\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.311419\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.308305\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.315221\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.322069\n",
            "resetting env. episode 709.000000, reward total was -20.000000. running mean: -20.318849\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.315660\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -20.302503\n",
            "resetting env. episode 712.000000, reward total was -19.000000. running mean: -20.289478\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.296584\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.303618\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.310582\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.317476\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.324301\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.321058\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.327847\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.334569\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -20.331223\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.337911\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.344532\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.341087\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.347676\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.354199\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.360657\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.357050\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.363480\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.359845\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.356247\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.362684\n",
            "resetting env. episode 733.000000, reward total was -17.000000. running mean: -20.329057\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.325767\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.322509\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.329284\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.335991\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.342631\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.349205\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.345713\n",
            "resetting env. episode 741.000000, reward total was -18.000000. running mean: -20.322256\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.319033\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.325843\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.332584\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.339259\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.335866\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.342507\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.349082\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.355592\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.362036\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.368415\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.364731\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.361084\n",
            "resetting env. episode 754.000000, reward total was -19.000000. running mean: -20.347473\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.353998\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.350458\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.356954\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.363384\n",
            "resetting env. episode 759.000000, reward total was -19.000000. running mean: -20.349750\n",
            "resetting env. episode 760.000000, reward total was -20.000000. running mean: -20.346253\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.352790\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.349262\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -20.345770\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.342312\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.348889\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.335400\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.342046\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.348626\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.355139\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.361588\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.367972\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.364292\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -20.350649\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.357143\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.363571\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -20.349936\n",
            "resetting env. episode 777.000000, reward total was -19.000000. running mean: -20.336436\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.333072\n",
            "resetting env. episode 779.000000, reward total was -16.000000. running mean: -20.289741\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.296844\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -20.293875\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.290937\n",
            "resetting env. episode 783.000000, reward total was -18.000000. running mean: -20.268027\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.275347\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.282594\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.279768\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.286970\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.294100\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.301159\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.298148\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.305166\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.302115\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.309093\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.316002\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.322842\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.329614\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.336318\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -20.322955\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.319725\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.326528\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.333263\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.339930\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -20.336531\n",
            "resetting env. episode 804.000000, reward total was -19.000000. running mean: -20.323165\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -20.309934\n",
            "resetting env. episode 806.000000, reward total was -20.000000. running mean: -20.306834\n",
            "resetting env. episode 807.000000, reward total was -19.000000. running mean: -20.293766\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -20.280828\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.278020\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.285240\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.292387\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.289464\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.296569\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.303603\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.310567\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.317462\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.314287\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.321144\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.327933\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.324653\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.331407\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.338093\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -20.334712\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -20.321365\n",
            "resetting env. episode 825.000000, reward total was -21.000000. running mean: -20.328151\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.334870\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.341521\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.348106\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.344625\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.341178\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.347767\n",
            "resetting env. episode 832.000000, reward total was -19.000000. running mean: -20.334289\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.340946\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.337537\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -20.334161\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.330820\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.327511\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.334236\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.340894\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.337485\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.344110\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.330669\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.337362\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.333989\n",
            "resetting env. episode 845.000000, reward total was -19.000000. running mean: -20.320649\n",
            "resetting env. episode 846.000000, reward total was -20.000000. running mean: -20.317442\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.324268\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.321025\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.327815\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.334537\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.331191\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.327880\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.334601\n",
            "resetting env. episode 854.000000, reward total was -20.000000. running mean: -20.331255\n",
            "resetting env. episode 855.000000, reward total was -19.000000. running mean: -20.317942\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.324763\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.321515\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.328300\n",
            "resetting env. episode 859.000000, reward total was -18.000000. running mean: -20.305017\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.311967\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.308847\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.315759\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.322601\n",
            "resetting env. episode 864.000000, reward total was -19.000000. running mean: -20.309375\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.316281\n",
            "resetting env. episode 866.000000, reward total was -19.000000. running mean: -20.303118\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.310087\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.306986\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.313917\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.310777\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.317670\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.314493\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.321348\n",
            "resetting env. episode 874.000000, reward total was -19.000000. running mean: -20.308135\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.305053\n",
            "resetting env. episode 876.000000, reward total was -19.000000. running mean: -20.292003\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.299083\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.296092\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.293131\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.300200\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.307198\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.314126\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.320984\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.327775\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.324497\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.331252\n",
            "resetting env. episode 887.000000, reward total was -20.000000. running mean: -20.327939\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.334660\n",
            "resetting env. episode 889.000000, reward total was -19.000000. running mean: -20.321313\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.328100\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.334819\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.341471\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.348056\n",
            "resetting env. episode 894.000000, reward total was -18.000000. running mean: -20.324576\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.331330\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -20.328017\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.334736\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.341389\n",
            "resetting env. episode 899.000000, reward total was -18.000000. running mean: -20.317975\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.324795\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.331547\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.338232\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.334850\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.331501\n",
            "resetting env. episode 905.000000, reward total was -18.000000. running mean: -20.308186\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.315104\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.311953\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.318834\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.305645\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.312589\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.309463\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -20.306368\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.313305\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.320172\n",
            "resetting env. episode 915.000000, reward total was -19.000000. running mean: -20.306970\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.313900\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.310761\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.307654\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.304577\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.311531\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.318416\n",
            "resetting env. episode 922.000000, reward total was -19.000000. running mean: -20.305232\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.312180\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.309058\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.315967\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.312808\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.309679\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -20.296583\n",
            "resetting env. episode 929.000000, reward total was -19.000000. running mean: -20.283617\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.290781\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.297873\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.304894\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.311845\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -20.308727\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.305639\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.302583\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.299557\n",
            "resetting env. episode 938.000000, reward total was -18.000000. running mean: -20.276562\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.273796\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.281058\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.288248\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.285365\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.292511\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.299586\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.306590\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.313525\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.310389\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.307285\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.314213\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.321070\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.327860\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.334581\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.341235\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.347823\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.344345\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.340901\n",
            "resetting env. episode 957.000000, reward total was -19.000000. running mean: -20.327492\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.314217\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.311075\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.317964\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.314785\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.311637\n",
            "resetting env. episode 963.000000, reward total was -19.000000. running mean: -20.298521\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.305535\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.312480\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.309355\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.306262\n",
            "resetting env. episode 968.000000, reward total was -19.000000. running mean: -20.293199\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.300267\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.307264\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -20.294192\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.301250\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.308237\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.305155\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.312103\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.318982\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.325793\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.332535\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.339209\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.345817\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.342359\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -20.338935\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.345546\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.352091\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.358570\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.354984\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -20.341434\n",
            "resetting env. episode 988.000000, reward total was -19.000000. running mean: -20.328020\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.334740\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.341392\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.327978\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.324698\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.331452\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.338137\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.344756\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.351308\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -20.337795\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.334417\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.341073\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -20.327662\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.334386\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.331042\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.337731\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.344354\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.350910\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.357401\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.363827\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.360189\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.366587\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.362921\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.369292\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.375599\n",
            "resetting env. episode 1013.000000, reward total was -19.000000. running mean: -20.361843\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -20.358225\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.354642\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.351096\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.347585\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.354109\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.360568\n",
            "resetting env. episode 1020.000000, reward total was -19.000000. running mean: -20.346962\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.343493\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.340058\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.346657\n",
            "resetting env. episode 1024.000000, reward total was -18.000000. running mean: -20.323191\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.329959\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.326659\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.333393\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -20.320059\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.316858\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.313690\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.320553\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.327347\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.324074\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.330833\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.327525\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.334249\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -20.330907\n",
            "resetting env. episode 1038.000000, reward total was -19.000000. running mean: -20.317598\n",
            "resetting env. episode 1039.000000, reward total was -20.000000. running mean: -20.314422\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.321278\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.318065\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.324884\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.331635\n",
            "resetting env. episode 1044.000000, reward total was -18.000000. running mean: -20.308319\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.305236\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.302183\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.299162\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.296170\n",
            "resetting env. episode 1049.000000, reward total was -16.000000. running mean: -20.253208\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.260676\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.268069\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.275389\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.282635\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.289808\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.296910\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.293941\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.301002\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.307992\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.314912\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -20.311763\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -20.298645\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.305659\n",
            "resetting env. episode 1063.000000, reward total was -18.000000. running mean: -20.282602\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.289776\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.296878\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.303910\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.310870\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.307762\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -20.294684\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.291737\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.288820\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -20.275932\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.283172\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -20.280341\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.287537\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.294662\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.301715\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.308698\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.315611\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.322455\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.329231\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -20.325938\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.332679\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.339352\n",
            "resetting env. episode 1085.000000, reward total was -18.000000. running mean: -20.315959\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.322799\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.329571\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.336275\n",
            "resetting env. episode 1089.000000, reward total was -18.000000. running mean: -20.312912\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.319783\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.326586\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.333320\n",
            "resetting env. episode 1093.000000, reward total was -19.000000. running mean: -20.319986\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -20.306787\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.313719\n",
            "resetting env. episode 1096.000000, reward total was -19.000000. running mean: -20.300582\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.307576\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.304500\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.311455\n",
            "resetting env. episode 1100.000000, reward total was -19.000000. running mean: -20.298340\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.305357\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -20.282303\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -20.279480\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.286686\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -20.283819\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.290981\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.288071\n",
            "resetting env. episode 1108.000000, reward total was -20.000000. running mean: -20.285190\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.292338\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.289415\n",
            "resetting env. episode 1111.000000, reward total was -18.000000. running mean: -20.266521\n",
            "resetting env. episode 1112.000000, reward total was -18.000000. running mean: -20.243855\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.251417\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.258903\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.266314\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.273651\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.280914\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.288105\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.295224\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.302272\n",
            "resetting env. episode 1121.000000, reward total was -18.000000. running mean: -20.279249\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.276456\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.283692\n",
            "resetting env. episode 1124.000000, reward total was -18.000000. running mean: -20.260855\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.258246\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.255664\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -20.253107\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.260576\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.267970\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.275291\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -20.262538\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.269912\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -20.267213\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.274541\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.281796\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.288978\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.296088\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.303127\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.310096\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.316995\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.323825\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -20.320587\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -20.317381\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.314207\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.321065\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.327854\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -20.324576\n",
            "resetting env. episode 1148.000000, reward total was -19.000000. running mean: -20.311330\n",
            "resetting env. episode 1149.000000, reward total was -20.000000. running mean: -20.308217\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.305135\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.312083\n",
            "resetting env. episode 1152.000000, reward total was -19.000000. running mean: -20.298962\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.305973\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.312913\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.319784\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.326586\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.323320\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.330087\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.336786\n",
            "resetting env. episode 1160.000000, reward total was -19.000000. running mean: -20.323418\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.330184\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.336882\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -20.323513\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.330278\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.326976\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.333706\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.340369\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.346965\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.353495\n",
            "resetting env. episode 1170.000000, reward total was -20.000000. running mean: -20.349960\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.356461\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.362896\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.359267\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -20.355675\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.362118\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.358497\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -20.344912\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.351463\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.357948\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.364368\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.370725\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.377018\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.383247\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.389415\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.395521\n",
            "resetting env. episode 1186.000000, reward total was -19.000000. running mean: -20.381566\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.387750\n",
            "resetting env. episode 1188.000000, reward total was -19.000000. running mean: -20.373872\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.380134\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.376332\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.382569\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.388743\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.394856\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.390907\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.396998\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.393028\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -20.389098\n",
            "resetting env. episode 1198.000000, reward total was -19.000000. running mean: -20.375207\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.381455\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.377640\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.373864\n",
            "resetting env. episode 1202.000000, reward total was -19.000000. running mean: -20.360125\n",
            "resetting env. episode 1203.000000, reward total was -19.000000. running mean: -20.346524\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.353059\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.359528\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.355933\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.362374\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.368750\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -20.355062\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.361512\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.357897\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.364318\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.370675\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.366968\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.373298\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -20.359565\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -20.345969\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.352510\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -20.348985\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.345495\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.352040\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.358519\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -20.344934\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -20.331485\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.338170\n",
            "resetting env. episode 1226.000000, reward total was -19.000000. running mean: -20.324788\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.331540\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.338225\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.344843\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.351394\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.357880\n",
            "resetting env. episode 1232.000000, reward total was -18.000000. running mean: -20.334302\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.330959\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.337649\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.344273\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.350830\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.357322\n",
            "resetting env. episode 1238.000000, reward total was -18.000000. running mean: -20.333748\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -20.330411\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -20.317107\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.313936\n",
            "resetting env. episode 1242.000000, reward total was -18.000000. running mean: -20.290796\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -20.277888\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.285109\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.292258\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -20.289336\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -20.276442\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.283678\n",
            "resetting env. episode 1249.000000, reward total was -18.000000. running mean: -20.260841\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.268233\n",
            "resetting env. episode 1251.000000, reward total was -19.000000. running mean: -20.255550\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.252995\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.260465\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.267860\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.265182\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.262530\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.269905\n",
            "resetting env. episode 1258.000000, reward total was -19.000000. running mean: -20.257206\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.264634\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.261987\n",
            "resetting env. episode 1261.000000, reward total was -19.000000. running mean: -20.249367\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.246874\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.254405\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -20.251861\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.259342\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -20.256749\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.254181\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.251640\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.259123\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.256532\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.263967\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.261327\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.258714\n",
            "resetting env. episode 1274.000000, reward total was -18.000000. running mean: -20.236127\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.233765\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.231428\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.229113\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.236822\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.244454\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.242009\n",
            "resetting env. episode 1281.000000, reward total was -19.000000. running mean: -20.229589\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -20.217293\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -20.215121\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.212969\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.220840\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -20.208631\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -20.206545\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.204479\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.212435\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.210310\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.218207\n",
            "resetting env. episode 1292.000000, reward total was -19.000000. running mean: -20.206025\n",
            "resetting env. episode 1293.000000, reward total was -17.000000. running mean: -20.173965\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.172225\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.180503\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.188698\n",
            "resetting env. episode 1297.000000, reward total was -20.000000. running mean: -20.186811\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.194943\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.202993\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.210964\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -20.198854\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.196865\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.204897\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.202848\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.210819\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -20.198711\n",
            "resetting env. episode 1307.000000, reward total was -19.000000. running mean: -20.186724\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -20.174857\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.173108\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.181377\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.189563\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.187668\n",
            "resetting env. episode 1313.000000, reward total was -19.000000. running mean: -20.175791\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.184033\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.182193\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.180371\n",
            "resetting env. episode 1317.000000, reward total was -19.000000. running mean: -20.168567\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.166881\n",
            "resetting env. episode 1319.000000, reward total was -21.000000. running mean: -20.175213\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -20.173460\n",
            "resetting env. episode 1321.000000, reward total was -21.000000. running mean: -20.181726\n",
            "resetting env. episode 1322.000000, reward total was -19.000000. running mean: -20.169909\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.178210\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.176427\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.174663\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.182917\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.191087\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -20.189176\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -20.187285\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.185412\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.183558\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.191722\n",
            "resetting env. episode 1333.000000, reward total was -20.000000. running mean: -20.189805\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -20.187907\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.196028\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.204068\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.202027\n",
            "resetting env. episode 1338.000000, reward total was -20.000000. running mean: -20.200007\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.208007\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.215926\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -20.203767\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.201730\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.209712\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.207615\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.215539\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.213384\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.221250\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.229037\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.236747\n",
            "resetting env. episode 1350.000000, reward total was -18.000000. running mean: -20.214379\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -20.212236\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -20.200113\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.208112\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.216031\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.223871\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.231632\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.229316\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.227023\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.234752\n",
            "resetting env. episode 1360.000000, reward total was -18.000000. running mean: -20.212405\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.210281\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.218178\n",
            "resetting env. episode 1363.000000, reward total was -19.000000. running mean: -20.205996\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -20.203936\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.211897\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.209778\n",
            "resetting env. episode 1367.000000, reward total was -18.000000. running mean: -20.187680\n",
            "resetting env. episode 1368.000000, reward total was -18.000000. running mean: -20.165803\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.174145\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.172404\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.180680\n",
            "resetting env. episode 1372.000000, reward total was -19.000000. running mean: -20.168873\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.167184\n",
            "resetting env. episode 1374.000000, reward total was -19.000000. running mean: -20.155512\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.163957\n",
            "resetting env. episode 1376.000000, reward total was -19.000000. running mean: -20.152318\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.150795\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.159287\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.167694\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -20.156017\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.144457\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -20.133012\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.141682\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.140265\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.148862\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.147374\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.145900\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.154441\n",
            "resetting env. episode 1389.000000, reward total was -20.000000. running mean: -20.152897\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.141368\n",
            "resetting env. episode 1391.000000, reward total was -18.000000. running mean: -20.119954\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.128754\n",
            "resetting env. episode 1393.000000, reward total was -18.000000. running mean: -20.107467\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.116392\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.115228\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.124076\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -20.122835\n",
            "resetting env. episode 1398.000000, reward total was -18.000000. running mean: -20.101607\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -20.100591\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.099585\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.098589\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -20.097603\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.106627\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.115561\n",
            "resetting env. episode 1405.000000, reward total was -19.000000. running mean: -20.104405\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.113361\n",
            "resetting env. episode 1407.000000, reward total was -19.000000. running mean: -20.102228\n",
            "resetting env. episode 1408.000000, reward total was -16.000000. running mean: -20.061205\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.070593\n",
            "resetting env. episode 1410.000000, reward total was -19.000000. running mean: -20.059887\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.069289\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.078596\n",
            "resetting env. episode 1413.000000, reward total was -20.000000. running mean: -20.077810\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.077032\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.086261\n",
            "resetting env. episode 1416.000000, reward total was -20.000000. running mean: -20.085399\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.084545\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -20.083699\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.092862\n",
            "resetting env. episode 1420.000000, reward total was -19.000000. running mean: -20.081934\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.091114\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -20.080203\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.089401\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.098507\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.107522\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -20.106447\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.115382\n",
            "resetting env. episode 1428.000000, reward total was -18.000000. running mean: -20.094228\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.093286\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.092353\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.101430\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.110416\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.109311\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.108218\n",
            "resetting env. episode 1435.000000, reward total was -20.000000. running mean: -20.107136\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.116065\n",
            "resetting env. episode 1437.000000, reward total was -19.000000. running mean: -20.104904\n",
            "resetting env. episode 1438.000000, reward total was -19.000000. running mean: -20.093855\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.102916\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.111887\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -20.100768\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.109761\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.118663\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.127477\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.136202\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.144840\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.143391\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.141957\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.150538\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.159032\n",
            "resetting env. episode 1451.000000, reward total was -17.000000. running mean: -20.127442\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.136168\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.134806\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.133458\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -20.142123\n",
            "resetting env. episode 1456.000000, reward total was -19.000000. running mean: -20.130702\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.139395\n",
            "resetting env. episode 1458.000000, reward total was -19.000000. running mean: -20.128001\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -20.116721\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.125554\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.124298\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.133055\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.131725\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.130408\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.129104\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.127813\n",
            "resetting env. episode 1467.000000, reward total was -19.000000. running mean: -20.116534\n",
            "resetting env. episode 1468.000000, reward total was -19.000000. running mean: -20.105369\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -20.104315\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.113272\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.122139\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.130918\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.129609\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.138313\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.136930\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.145560\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.154105\n",
            "resetting env. episode 1478.000000, reward total was -21.000000. running mean: -20.162564\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.170938\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.179229\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.187436\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.195562\n",
            "resetting env. episode 1483.000000, reward total was -19.000000. running mean: -20.183606\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -20.171770\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.180053\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.188252\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.196370\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.194406\n",
            "resetting env. episode 1489.000000, reward total was -19.000000. running mean: -20.182462\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.180637\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.178831\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -20.167043\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.175372\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.183618\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.191782\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.199864\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.207866\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.215787\n",
            "resetting env. episode 1499.000000, reward total was -19.000000. running mean: -20.203629\n",
            "resetting env. episode 1500.000000, reward total was -17.000000. running mean: -20.171593\n",
            "CPU times: user 1h 12min 18s, sys: 33min 16s, total: 1h 45min 35s\n",
            "Wall time: 54min 40s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "41e7d062-abbf-40c2-a3fd-1a4b873d4577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}